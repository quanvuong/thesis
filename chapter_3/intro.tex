
\newcommand{\name}{\text{DASCO}} 

Offline reinforcement learning (RL) algorithms aim to extract policies from datasets of previously logged experience. The promise of offline RL is to extract \textit{decision making engines} from existing data \cite{levine2020offline}. Such promise is especially appealing in domains where data collection is expensive or dangerous, but large amounts of data may already exists (e.g., robotics, autonomous driving, task-oriented dialog systems). Real-world datasets often consist of both expert and sub-optimal behaviors for the task of interest and also include potentially unrelated behavior corresponding to other tasks. While not all behaviors in the dataset are relevant for solving the task of interest, even sub-optimal trajectories can provide an RL algorithm with some useful information. In principle, if offline RL algorithms can combine segments of useful behavior spread across multiple sub-optimal trajectories together, the combined segments can then perform better than any behavior observed in the dataset. 

Effective offline RL requires estimating the value of actions other than those that were taken in the dataset, so as to pick actions that are better than the actions selected by the behavior policy. However, this requirement introduces a fundamental tension: the offline RL method must generalize to new actions, but it should not attempt to use actions in the Bellman backup for which the value simply cannot be estimated using the provided data. 
These are often referred to in the literature as out-of-distribution (OOD) actions~\cite{bear}. 
While a wide variety of methods have been proposed to constrain offline RL to avoid OOD actions~\cite{kostrikov2021offline, bcq, agarwal2019optimistic}, the formulation and enforcement of such constraints can be challenging, and might introduce considerable complexity, such as the need to explicitly estimate the behavior policy~\cite{brac} or evaluate high-dimensional integrals~\cite{kumar2020conservative}. Generative adversarial networks (GANs) in principle offer an appealing and simple solution: use the discriminator to estimate whether an action is in-distribution, and train the policy as the ``generator'' in the GAN formulation to fool this discriminator. Although some prior works have proposed variants on this approach~\cite{brac}, it has been proven difficult in practice as GANs can already suffer from instability when the discriminator is too powerful. Forcing the generator (i.e., the policy) to simultaneously \emph{both} maximize reward and fool the discriminator only exacerbates the issue of an overpowered discriminator.

We propose a novel solution that enables the effective use of GANs in offline RL, in the process not only mitigating the above challenge but also providing a more appealing form of support constraint that leads to improved performance. Our key observation is that the generative distribution in GANs can be split into \emph{two} separate distributions, one that represents the ``good parts'' of the data distribution and becomes the final learned policy, and an auxiliary generator that becomes the policy's complement, such that the mixture of the two is equal to the data distribution. This formulation removes the tension between maximizing rewards and matching the data distribution perfectly: as long as the learned policy is within the \emph{support} of the data distribution, the complement will pick up the slack and model the ``remainder" of the data distribution, allowing the two generators together to perfectly fool the discriminator. If however the policy ventures outside of the support of the data, the second generator cannot compensate for this mistake, and the discriminator will push the policy back inside the support. 
We name our method \name, for \textbf{D}ual-Generator \textbf{A}dversarial \textbf{S}upport \textbf{C}onstrained \textbf{O}ffline RL.

Experimentally, we demonstrate the benefits of our approach, \name{},
on standard benchmark tasks. For offline datasets that consist of a combination of expert, sub-optimal and noisy data,
our method outperforms distribution-constrained offline RL methods by a large margin.  
