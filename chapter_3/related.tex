\section{Related Work}

Combining behaviors from sub-optimal trajectories to obtain high-performing policies is a central promise of offline RL. During offline training, querying the value function on unseen actions often leads to value over-estimation and unrecoverable collapse in learning progress. To avoid querying the value functions on out-of-distribution actions, existing methods encourage the learned policies to match the distribution of the dataset generation policies. This principle has been realized with a variety of practical algorithms ~\cite{jaques2019way,brac,peng2019awr,siegel2020keep,brac,kumar2019stabilizing, kostrikov2021offline,kostrikov2021offlineb,wang2020critic,fujimoto2021minimalist, BAIL, furuta2022generalized, jang2022gptcritic, meng2022offline, daoudi2022density, liu2022robust}.

For example, by optimizing the policies with respect to a conservative lower bound of the value function estimate \cite{kumar2020conservative}, only optimizing the policies on actions contained in the dataset \cite{kostrikov2021offline}, or jointly optimizing the policy on the long-term return and a behavior cloning objective \cite{fujimoto2021minimalist}. While \textit{explicitly} enforcing distribution constraint by adding the behavior cloning objective allows for good performance on near-optimal data, 
%the Gym locomotion tasks or on the D4RL benchmarks, 
this approach fails to produce good trajectories on sub-optimal datasets~\cite{kostrikov2021offline}.
%the antmaze-large and antmaze-medium tasks \cite{kostrikov2021offline}, which require stitching to obtain good performance \cite{d4rl}. 
Methods that \textit{implicitly} enforce distribution constraints, such as CQL and IQL, have seen more successes on such datasets.
%the antmaze-large and antmaze-medium tasks. 
However, they still struggle to produce near-optimal trajectories when the actions of the dataset generation policies are corrupted with noise or systematic biases (a result we demonstrate in Section~\ref{sec:exp}).
%%SL.5.18: The above discussion is reasonable, but it sets the expectation for the reader that the paper will demonstrate a significant improvement in terms of D4RL ant maze performance. If that's intended, then great. But if the results will be less amazing there, maybe revise this sentence a bit to set more realistic expectations?

However, enforcing distribution constraints  to avoid value over-estimation may not be necessary. It is sufficient to ensure the learned policies do not produce actions that are too unlikely under the dataset generation policy. That is, it is not necessary for the learned policy to fully \emph{cover} the data distribution, only to remain in-support~\cite{kumar2019stabilizing,kumar_blog,levine2020offline,brac,zhou2020plas,chen2022latent}. Unfortunately, previous methods that attempt to instantiate this principle into algorithms have not seen as much empirical success as algorithms that penalize the policies for not matching the action distribution of the behavior policies. In this paper, we propose a new GAN-based offline RL algorithm whose use of dual generators naturally induce support constraint and has competitive performance with recent offline RL methods. In a number of prior works, GANs have been used in the context of imitation learning to learn from expert data~\cite{gail,infogail,intentiongan,zhihanliu}. In this work, we show that dual-generator GANs can be used to learn from sub-optimal data in the context of offline RL.
% distribution constraint offline RL algorithms.
%%SL.5.18: Kind of a nitpick, but "distribution constraint offline RL algorithms" is very overloaded the way it is used, because it's debatable whether pessimism-based methods like CQL are really "distribution constraint" (for the purpose of this discussion, they actually are, but some readers might not understand this and object).
% What is especially puzzling is that existing support constraint offline RL algorithms do not perform as well as distribution constraint algorithms when learning from data sets that requires stitching, the scenario where we expect support constrained methods to clearly demonstrate their benefits. 
%%SL.5.18: While a valid point, this also sets an uncomfortable expectation that the paper will resolve this mystery (when in fact the paper doesn't do that, it just proposes a new method that works better). Maybe revise the above?

% TODO: add related work for GAN \textbf{GAN}