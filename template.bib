@book{Martin_1983,
  title     = {Human Brain Evolution in an Ecological Context},
  publisher = {American Museum of Natural History},
  author    = {Martin, Robert D.},
  year      = {1983}
},

@article{Rilling_Insel_1999,
  title   = {The primate neocortex in comparative perspective using magnetic resonance imaging},
  volume  = {37},
  issn    = {0047-2484},
  number  = {2},
  journal = {Journal of Human Evolution},
  author  = {Rilling, James K. and Insel, Thomas R.},
  year    = {1999},
  pages   = {191--223}
}

@article{10.1093/mind/LIX.236.433,
  author  = {TURING, A. M.},
  title   = {{I.â€”COMPUTING MACHINERY AND INTELLIGENCE}},
  journal = {Mind},
  volume  = {LIX},
  number  = {236},
  pages   = {433-460},
  year    = {1950},
  month   = {10},
  issn    = {0026-4423},
  doi     = {10.1093/mind/LIX.236.433},
  url     = {https://doi.org/10.1093/mind/LIX.236.433},
  eprint  = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf}
}

@article{rakelly2019efficient,
  title   = {Efficient off-policy meta-reinforcement learning via probabilistic context variables},
  author  = {Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey},
  journal = {arXiv preprint arXiv:1903.08254},
  year    = {2019}
}

@inproceedings{fujimoto2019off,
  title     = {Off-Policy Deep Reinforcement Learning without Exploration},
  author    = {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = {International Conference on Machine Learning},
  pages     = {2052--2062},
  year      = {2019}
}

@article{siegel2020keep,
  title   = {Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning},
  author  = {Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:2002.08396},
  year    = {2020}
}

@inproceedings{zintgraf2020varibad,
  title     = {VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},
  author    = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
  booktitle = {International Conference on Learning Representation (ICLR)},
  year      = {2020}
}

@article{agarwal2019optimistic,
  title   = {An Optimistic Perspective on Offline Reinforcement Learning},
  author  = {Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  journal = {arXiv preprint arXiv:1907.04543},
  year    = {2019}
}

@inproceedings{kumar2019stabilizing,
  title     = {Stabilizing off-policy q-learning via bootstrapping error reduction},
  author    = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11761--11771},
  year      = {2019}
}

@inproceedings{todorov2012mujoco,
  title        = {Mujoco: A physics engine for model-based control},
  author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle    = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages        = {5026--5033},
  year         = {2012},
  organization = {IEEE}
}

@article{haarnoja2018soft,
  title   = {Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author  = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal = {arXiv preprint arXiv:1801.01290},
  year    = {2018}
}

@article{hermans2017defense,
  title   = {In defense of the triplet loss for person re-identification},
  author  = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  journal = {arXiv preprint arXiv:1703.07737},
  year    = {2017}
}

@inproceedings{chua2018deep,
  title     = {Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author    = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {4754--4765},
  year      = {2018}
}

@article{kingma2013auto,
  title   = {Auto-encoding variational bayes},
  author  = {Kingma, Diederik P and Welling, Max},
  journal = {arXiv preprint arXiv:1312.6114},
  year    = {2013}
}

@inproceedings{van2016deep,
  title     = {Deep reinforcement learning with double q-learning},
  author    = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle = {Thirtieth AAAI conference on artificial intelligence},
  year      = {2016}
}

@article{cabi2020ScalingDR,
  title   = {Scaling data-driven robotics with reward sketching and batch reinforcement learning.},
  author  = {Serkan Cabi and Sergio G{\'o}mez Colmenarejo and Alexander Novikov and Ksenia Konyushkova and Scott Reed and Rae Jeong and Konrad Zolna and Yusuf Aytar and David Budden and Mel Vecer{\'i}k and Oleg Sushkov and David J. P. Barker and Jonathan Scholz and Misha Denil and Nando de Freitas and Ziyu Wang},
  journal = {arXiv: Robotics},
  year    = {2020}
}

@inproceedings{IL_Pieter,
  author    = {Abbeel, Pieter and Ng, Andrew Y.},
  title     = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning},
  series    = {ICML '04},
  year      = {2004},
  isbn      = {1-58113-838-5},
  location  = {Banff, Alberta, Canada},
  pages     = {1--},
  url       = {http://doi.acm.org/10.1145/1015330.1015430},
  doi       = {10.1145/1015330.1015430},
  acmid     = {1015430},
  publisher = {ACM},
  address   = {New York, NY, USA}
} 

@article{GAIL,
  author        = {Jonathan Ho and
                   Stefano Ermon},
  title         = {Generative Adversarial Imitation Learning},
  journal       = {CoRR},
  volume        = {abs/1606.03476},
  year          = {2016},
  url           = {http://arxiv.org/abs/1606.03476},
  archiveprefix = {arXiv},
  eprint        = {1606.03476},
  timestamp     = {Mon, 13 Aug 2018 16:47:10 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/HoE16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{LearnToReinforceLearnWang2016,
  author        = {Jane X. Wang and
                   Zeb Kurth{-}Nelson and
                   Dhruva Tirumala and
                   Hubert Soyer and
                   Joel Z. Leibo and
                   R{\'{e}}mi Munos and
                   Charles Blundell and
                   Dharshan Kumaran and
                   Matthew Botvinick},
  title         = {Learning to reinforcement learn},
  journal       = {CoRR},
  volume        = {abs/1611.05763},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.05763},
  archiveprefix = {arXiv},
  eprint        = {1611.05763},
  timestamp     = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/WangKTSLMBKB16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{MAMLFinn2017,
  author        = {Chelsea Finn and
                   Pieter Abbeel and
                   Sergey Levine},
  title         = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  journal       = {CoRR},
  volume        = {abs/1703.03400},
  year          = {2017},
  url           = {http://arxiv.org/abs/1703.03400},
  archiveprefix = {arXiv},
  eprint        = {1703.03400},
  timestamp     = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/FinnAL17},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{CAVIA,
  title     = {Fast Context Adaptation via Meta-Learning},
  author    = {Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {7693--7702},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR}
}

@incollection{HiddenParamTaylor2017,
  title     = {Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes},
  author    = {Killian, Taylor W and Daulton, Samuel and Konidaris, George and Doshi-Velez, Finale},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {6250--6261},
  year      = {2017},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.nips.cc/paper/7205-robust-and-efficient-transfer-learning-with-hidden-parameter-markov-decision-processes.pdf}
}

@article{benchmarkingWang2019,
  author  = {Tingwu Wang and
             Xuchan Bao and
             Ignasi Clavera and
             Jerrick Hoang and
             Yeming Wen and
             Eric Langlois and
             Shunshi Zhang and
             Guodong Zhang and
             Pieter Abbeel and
             Jimmy Ba},
  title   = {Benchmarking Model-Based Reinforcement Learning},
  journal = {CoRR},
  volume  = {abs/1907.02057},
  year    = {2019},
  url     = {http://arxiv.org/abs/1907.02057}
}

@article{ActorMimicParisotto2015,
  title   = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
  author  = {Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  journal = {CoRR},
  year    = {2015},
  volume  = {abs/1511.06342}
}

@article{caruana1997multitask,
  title     = {Multitask learning},
  author    = {Caruana, Rich},
  journal   = {Machine learning},
  volume    = {28},
  number    = {1},
  pages     = {41--75},
  year      = {1997},
  publisher = {Springer}
}

@article{yang2020multi,
  title   = {Multi-Task Reinforcement Learning with Soft Modularization},
  author  = {Yang, Ruihan and Xu, Huazhe and Wu, Yi and Wang, Xiaolong},
  journal = {arXiv preprint arXiv:2003.13661},
  year    = {2020}
}

@article{espeholt2018impala,
  title   = {Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author  = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal = {arXiv preprint arXiv:1802.01561},
  year    = {2018}
}

@article{yumulti,
  title  = {Multi-Task Reinforcement Learning without Interference},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  url    = {https://optrl2019.github.io/assets/accepted_papers/16.pdf},
  year    = {2019}
}

@article{rusu2015policy,
  title   = {Policy distillation},
  author  = {Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal = {arXiv preprint arXiv:1511.06295},
  year    = {2015}
}

@article{levine2016end,
  title     = {End-to-end training of deep visuomotor policies},
  author    = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal   = {The Journal of Machine Learning Research},
  volume    = {17},
  number    = {1},
  pages     = {1334--1373},
  year      = {2016},
  publisher = {JMLR. org}
}

@inproceedings{teh2017distral,
  title     = {Distral: Robust multitask reinforcement learning},
  author    = {Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {4496--4506},
  year      = {2017}
}

@article{ghosh2017divide,
  title   = {Divide-and-conquer reinforcement learning},
  author  = {Ghosh, Dibya and Singh, Avi and Rajeswaran, Aravind and Kumar, Vikash and Levine, Sergey},
  journal = {arXiv preprint arXiv:1711.09874},
  year    = {2017}
}

@article{czarnecki2019distilling,
  title   = {Distilling policy distillation},
  author  = {Czarnecki, Wojciech Marian and Pascanu, Razvan and Osindero, Simon and Jayakumar, Siddhant M and Swirszcz, Grzegorz and Jaderberg, Max},
  journal = {arXiv preprint arXiv:1902.02186},
  year    = {2019}
}

@article{fakoor2019meta,
  title   = {Meta-Q-Learning},
  author  = {Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J},
  journal = {arXiv preprint arXiv:1910.00125},
  year    = {2019}
}

@article{humplik2019meta,
  title   = {Meta reinforcement learning as task inference},
  author  = {Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A and Teh, Yee Whye and Heess, Nicolas},
  journal = {arXiv preprint arXiv:1905.06424},
  year    = {2019}
}

@article{lan2019meta,
  title   = {Meta reinforcement learning with task embedding and shared policy},
  author  = {Lan, Lin and Li, Zhenguo and Guan, Xiaohong and Wang, Pinghui},
  journal = {arXiv preprint arXiv:1905.06527},
  year    = {2019}
}

@article{saemundsson2018meta,
  title   = {Meta reinforcement learning with latent variable gaussian processes},
  author  = {S{\ae}mundsson, Steind{\'o}r and Hofmann, Katja and Deisenroth, Marc Peter},
  journal = {arXiv preprint arXiv:1803.07551},
  year    = {2018}
}

@article{duan2016rl,
  title   = {RL{\textdollar}{\^{}}2{\textdollar}: Fast reinforcement learning via slow reinforcement learning},
  author  = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1611.02779},
  year    = {2016}
}

@article{chu2017cyclegan,
  title   = {Cyclegan, a master of steganography},
  author  = {Chu, Casey and Zhmoginov, Andrey and Sandler, Mark},
  journal = {arXiv preprint arXiv:1712.02950},
  year    = {2017}
}

@article{chen2019bail,
  title   = {BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning},
  author  = {Chen, Xinyue and Zhou, Zijian and Wang, Zheng and Wang, Che and Wu, Yanqiu and Deng, Qing and Ross, Keith},
  journal = {arXiv preprint arXiv:1910.12179},
  year    = {2019}
}

@article{finn2017maml,
  author        = {Chelsea Finn and
                   Pieter Abbeel and
                   Sergey Levine},
  title         = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  journal       = {CoRR},
  volume        = {abs/1703.03400},
  year          = {2017},
  url           = {http://arxiv.org/abs/1703.03400},
  archiveprefix = {arXiv},
  eprint        = {1703.03400},
  timestamp     = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/FinnAL17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{nichol2018Reptile,
  author        = {Alex Nichol and
                   Joshua Achiam and
                   John Schulman},
  title         = {On First-Order Meta-Learning Algorithms},
  journal       = {CoRR},
  volume        = {abs/1803.02999},
  year          = {2018},
  url           = {http://arxiv.org/abs/1803.02999},
  archiveprefix = {arXiv},
  eprint        = {1803.02999},
  timestamp     = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{houthooft2018evolvedpg,
  author        = {Rein Houthooft and
                   Richard Y. Chen and
                   Phillip Isola and
                   Bradly C. Stadie and
                   Filip Wolski and
                   Jonathan Ho and
                   Pieter Abbeel},
  title         = {Evolved Policy Gradients},
  journal       = {CoRR},
  volume        = {abs/1802.04821},
  year          = {2018},
  url           = {http://arxiv.org/abs/1802.04821},
  archiveprefix = {arXiv},
  eprint        = {1802.04821},
  timestamp     = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1802-04821.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@book{pearl_2009,
  place     = {Cambridge},
  title     = {Causality},
  doi       = {10.1017/CBO9780511803161},
  publisher = {Cambridge University Press},
  author    = {Pearl, Judea},
  year      = {2009}
}

@book{Peters2017,
  author    = {Peters, J. and Janzing, D. and Sch\"olkopf, B.},
  title     = {Elements of Causal Inference: Foundations and Learning Algorithms},
  address   = {Cambridge, MA, USA},
  publisher = {MIT Press},
  year      = {2017}
}

@inproceedings{hessel2019multi,
  title     = {Multi-task deep reinforcement learning with popart},
  author    = {Hessel, Matteo and Soyer, Hubert and Espeholt, Lasse and Czarnecki, Wojciech and Schmitt, Simon and van Hasselt, Hado},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {3796--3803},
  year      = {2019}
}

@inproceedings{impala2018,
  title     = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
  author    = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2018}
}

@inproceedings{de2019causal,
  title     = {Causal confusion in imitation learning},
  author    = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11693--11704},
  year      = {2019}
}

@article{xu2019can,
  title   = {What Can Neural Networks Reason About?},
  author  = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  journal = {arXiv preprint arXiv:1905.13211},
  year    = {2019}
}

@article{pearl2010introduction,
  title     = {An introduction to causal inference},
  author    = {Pearl, Judea},
  journal   = {The international journal of biostatistics},
  volume    = {6},
  number    = {2},
  year      = {2010},
  publisher = {De Gruyter}
}

@article{pearl2009causal,
  title     = {Causal inference in statistics: An overview},
  author    = {Pearl, Judea and others},
  journal   = {Statistics surveys},
  volume    = {3},
  pages     = {96--146},
  year      = {2009},
  publisher = {The author, under a Creative Commons Attribution License}
}

@article{scarselli2008graph,
  title     = {The graph neural network model},
  author    = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal   = {IEEE Transactions on Neural Networks},
  volume    = {20},
  number    = {1},
  pages     = {61--80},
  year      = {2008},
  publisher = {IEEE}
}

@article{wu2020comprehensive,
  title     = {A comprehensive survey on graph neural networks},
  author    = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2020},
  publisher = {IEEE}
}

@article{zhou2018graph,
  title   = {Graph neural networks: A review of methods and applications},
  author  = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal = {arXiv preprint arXiv:1812.08434},
  year    = {2018}
}

@inproceedings{d2019sharing,
  title     = {Sharing Knowledge in Multi-Task Deep Reinforcement Learning},
  author    = {D'Eramo, Carlo and Tateo, Davide and Bonarini, Andrea and Restelli, Marcello and Peters, Jan},
  booktitle = {International Conference on Learning Representations},
  year      = {2019}
}

@misc{xie2019selftraining,
  title         = {Self-training with Noisy Student improves ImageNet classification},
  author        = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le},
  year          = {2019},
  eprint        = {1911.04252},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{mobahi2020selfdistillation,
  title         = {Self-Distillation Amplifies Regularization in Hilbert Space},
  author        = {Hossein Mobahi and Mehrdad Farajtabar and Peter L. Bartlett},
  year          = {2020},
  eprint        = {2002.05715},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{sharma2020emergent,
  title         = {Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning},
  author        = {Archit Sharma and Michael Ahn and Sergey Levine and Vikash Kumar and Karol Hausman and Shixiang Gu},
  year          = {2020},
  eprint        = {2004.12974},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO}
}

@article{peng2019MCP,
  author        = {Xue Bin Peng and
                   Michael Chang and
                   Grace Zhang and
                   Pieter Abbeel and
                   Sergey Levine},
  title         = {{MCP:} Learning Composable Hierarchical Control with Multiplicative
                   Compositional Policies},
  journal       = {CoRR},
  volume        = {abs/1905.09808},
  year          = {2019},
  url           = {http://arxiv.org/abs/1905.09808},
  archiveprefix = {arXiv},
  eprint        = {1905.09808},
  timestamp     = {Wed, 29 May 2019 11:27:50 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1905-09808.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@book{sutton2018reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press}
}

@article{TD3,
  author        = {Scott Fujimoto and
                   Herke van Hoof and
                   David Meger},
  title         = {Addressing Function Approximation Error in Actor-Critic Methods},
  journal       = {CoRR},
  volume        = {abs/1802.09477},
  year          = {2018},
  url           = {http://arxiv.org/abs/1802.09477},
  archiveprefix = {arXiv},
  eprint        = {1802.09477},
  timestamp     = {Sat, 28 Sep 2019 00:58:01 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1802-09477.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{choi2019meta,
  title   = {Meta-amortized variational inference and learning},
  author  = {Choi, Kristy and Wu, Mike and Goodman, Noah and Ermon, Stefano},
  journal = {arXiv preprint arXiv:1902.01950},
  year    = {2019}
}

@inproceedings{geiger2012we,
  title        = {Are we ready for autonomous driving? the kitti vision benchmark suite},
  author       = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle    = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages        = {3354--3361},
  year         = {2012},
  organization = {IEEE}
}

@article{SpinningUp2018,
  author = {Achiam, Joshua},
  title  = {{Spinning Up in Deep Reinforcement Learning}},
  year   = {2018}
}

@inproceedings{Bengio2020A,
  title     = {A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
  author    = {Yoshua Bengio and Tristan Deleu and Nasim Rahaman and Nan Rosemary Ke and Sebastien Lachapelle and Olexa Bilaniuk and Anirudh Goyal and Christopher Pal},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=ryxWIgBFPS}
}


@article{metagenrl,
  title   = {Improving generalization in meta reinforcement learning using learned objectives},
  author  = {Kirsch, Louis and van Steenkiste, Sjoerd and Schmidhuber, J{\"u}rgen},
  journal = {arXiv preprint arXiv:1910.04098},
  year    = {2019}
}

@article{ddpg,
  title   = {Continuous control with deep reinforcement learning},
  author  = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal = {arXiv preprint arXiv:1509.02971},
  year    = {2015}
}

@article{fu2020d4rl,
  title   = {D4rl: Datasets for deep data-driven reinforcement learning},
  author  = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal = {arXiv preprint arXiv:2004.07219},
  year    = {2020}
}

@article{kumar2020conservative,
  title   = {Conservative Q-Learning for Offline Reinforcement Learning},
  author  = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal = {arXiv preprint arXiv:2006.04779},
  year    = {2020}
}

@article{roth2020revisiting,
  title   = {Revisiting training strategies and generalization performance in deep metric learning},
  author  = {Roth, Karsten and Milbich, Timo and Sinha, Samarth and Gupta, Prateek and Ommer, Bjoern and Cohen, Joseph Paul},
  journal = {arXiv preprint arXiv:2002.08473},
  year    = {2020}
}

@article{nair2020accelerating,
  title   = {Accelerating Online Reinforcement Learning with Offline Datasets},
  author  = {Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal = {arXiv preprint arXiv:2006.09359},
  year    = {2020}
}

@article{SPU,
  author        = {Quan Ho Vuong and
                   Yiming Zhang and
                   Keith W. Ross},
  title         = {Supervised Policy Update},
  journal       = {CoRR},
  volume        = {abs/1805.11706},
  year          = {2018},
  url           = {http://arxiv.org/abs/1805.11706},
  archiveprefix = {arXiv},
  eprint        = {1805.11706},
  timestamp     = {Thu, 06 Aug 2020 15:57:05 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1805-11706.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{PPO,
  author        = {John Schulman and
                   Filip Wolski and
                   Prafulla Dhariwal and
                   Alec Radford and
                   Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  journal       = {CoRR},
  volume        = {abs/1707.06347},
  year          = {2017},
  url           = {http://arxiv.org/abs/1707.06347},
  archiveprefix = {arXiv},
  eprint        = {1707.06347},
  timestamp     = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{SAC,
  author        = {Tuomas Haarnoja and
                   Aurick Zhou and
                   Pieter Abbeel and
                   Sergey Levine},
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                   with a Stochastic Actor},
  journal       = {CoRR},
  volume        = {abs/1801.01290},
  year          = {2018},
  url           = {http://arxiv.org/abs/1801.01290},
  archiveprefix = {arXiv},
  eprint        = {1801.01290},
  timestamp     = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{TRPO,
  author        = {John Schulman and
                   Sergey Levine and
                   Philipp Moritz and
                   Michael I. Jordan and
                   Pieter Abbeel},
  title         = {Trust Region Policy Optimization},
  journal       = {CoRR},
  volume        = {abs/1502.05477},
  year          = {2015},
  url           = {http://arxiv.org/abs/1502.05477},
  archiveprefix = {arXiv},
  eprint        = {1502.05477},
  timestamp     = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{SOP,
  title         = {Striving for Simplicity and Performance in Off-Policy DRL: Output Normalization and Non-Uniform Sampling},
  author        = {Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},
  year          = {2020},
  eprint        = {1910.02208},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@STRING{aistats = {AISTATS}}
@STRING{cvpr = {CVPR}}
@STRING{emnlp = {EMNLP}}
@STRING{focs = {FOCS}}
@STRING{iccv = {ICCV}}
@STRING{icdr = {ICDR}}
@STRING{icml = {ICML}}
@STRING{ijcv = {IJCV}}
@STRING{jmlr = {JMLR}}
@STRING{nc = {Neural Comput.}}
@STRING{nips = {NeurIPS}}
@STRING{pami = {IEEE Trans. PAMI}}
@STRING{sigir = {SIGIR}}
@STRING{stoc = {STOC}}
@STRING{socg = {SoCG}}
@STRING{uai = {UAI}}
@STRING{vissapp = {VISSAPP}}
@STRING{eccv = {ECCV}}
@STRING{acl = {ACL}}
@STRING{iclr = {ICLR}}
@STRING{icassp = {ICASSP}}
@STRING{asru = {ASRU}}
@STRING{machlearning = {Mach. Learn. J.}}
@STRING{aaai = {AAAI}}
@STRING{naacl = {NAACL}}
@STRING{iros = {IROS}}
@STRING{tacl = {TACL}}
@STRING{corl = {CoRL}}
@STRING{ijcai = {IJCAI}}
@STRING{aamas = {AAMAS}}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={arXiv preprint arXiv:2106.01345},
  year={2021}
}


@article{kostrikov2021offlineb,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@inproceedings{
mandlekar2021what,
title={What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
author={Ajay Mandlekar and Danfei Xu and Josiah Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Fei-Fei Li and Silvio Savarese and Yuke Zhu and Roberto Mart{\'\i}n-Mart{\'\i}n},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=JrsfBJtDFdI}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={arXiv preprint arXiv:2103.12021},
  year={2021}
}


@article{ebert2021bridge,
  title={Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets},
  author={Ebert, Frederik and Yang, Yanlai and Schmeckpeper, Karl and Bucher, Bernadette and Georgakis, Georgios and Daniilidis, Kostas and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2109.13396},
  year={2021}
}

@inproceedings{ettinger2021large,
  title={Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset},
  author={Ettinger, Scott and Cheng, Shuyang and Caine, Benjamin and Liu, Chenxi and Zhao, Hang and Pradhan, Sabeek and Chai, Yuning and Sapp, Ben and Qi, Charles R and Zhou, Yin and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9710--9719},
  year={2021}
}

@article{Rafailov2020LOMPO,
  title={Offline Reinforcement Learning from Images with Latent Space Models},
  author={Rafael Rafailov and Tianhe Yu and A. Rajeswaran and Chelsea Finn},
  journal={Learning for Decision Making and Control (L4DC)},
  year={2021},
}

@article{argenson2020model,
  title={Model-Based Offline Planning},
  author={Argenson, Arthur and Dulac-Arnold, Gabriel},
  journal={arXiv preprint arXiv:2008.05556},
  year={2020}
}

@article{brandfonbrener2021offline,
  title={Offline RL Without Off-Policy Evaluation},
  author={Brandfonbrener, David and Whitney, William F and Ranganath, Rajesh and Bruna, Joan},
  journal={arXiv preprint arXiv:2106.08909},
  year={2021}
}

@article{kidambi2020morel,
  title={MOReL: Model-Based Offline Reinforcement Learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={arXiv preprint arXiv:2005.05951},
  year={2020}
}

@article{matsushima2020deployment,
  title={Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization},
  author={Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang},
  journal={arXiv preprint arXiv:2006.03647},
  year={2020}
}

@inproceedings{
lee2021representation,
title={Representation Balancing Offline Model-based Reinforcement Learning},
author={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QpNz8r_Ri2Y}
}

@article{swazinna2020overcoming,
  title={Overcoming Model Bias for Robust Offline Deep Reinforcement Learning},
  author={Swazinna, Phillip and Udluft, Steffen and Runkler, Thomas},
  journal={arXiv preprint arXiv:2008.05533},
  year={2020}
}

@article{emmons2021rvs,
  title={RvS: What is Essential for Offline RL via Supervised Learning?},
  author={Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2112.10751},
  year={2021}
}

@article{rezaeifar2021offline,
  title={Offline reinforcement learning as anti-exploration},
  author={Rezaeifar, Shideh and Dadashi, Robert and Vieillard, Nino and Hussenot, L{\'e}onard and Bachem, Olivier and Pietquin, Olivier and Geist, Matthieu},
  journal={arXiv preprint arXiv:2106.06431},
  year={2021}
}

@article{zhou2020plas,
  title={PLAS: Latent Action Space for Offline Reinforcement Learning},
  author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},
  journal={arXiv preprint arXiv:2011.07213},
  year={2020}
}


@article{chen2022latent,
  title={Latent-Variable Advantage-Weighted Policy Optimization for Offline RL},
  author={Chen, Xi and Ghadirzadeh, Ali and Yu, Tianhe and Gao, Yuan and Wang, Jianhao and Li, Wenzhe and Liang, Bin and Finn, Chelsea and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2203.08949},
  year={2022}
}

@article{fujimoto2021minimalist,
  title={A Minimalist Approach to Offline Reinforcement Learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2106.06860},
  year={2021}
}

@article{wang2020critic,
  title={Critic regularized regression},
  author={Wang, Ziyu and Novikov, Alexander and {\.Z}o{\l}na, Konrad and Springenberg, Jost Tobias and Reed, Scott and Shahriari, Bobak and Siegel, Noah and Merel, Josh and Gulcehre, Caglar and Heess, Nicolas and others},
  journal={arXiv preprint arXiv:2006.15134},
  year={2020}
}


@article{yu2021combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2102.08363},
  year={2021}
}

@inproceedings{
kumar2022should,
title={Should I Run Offline Reinforcement Learning or Behavioral Cloning?},
author={Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AP1MKT37rJ}
}


@article{zhang2019all,
  title={Are all layers created equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@article{agarwal2021precipice,
  title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel
          and Courville, Aaron and Bellemare, Marc G},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}


@inproceedings{snoek2015scalable,
  title={Scalable bayesian optimization using deep neural networks},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={2171--2180},
  year={2015}
}


@article{schrittwieser2021online,
  title={Online and offline reinforcement learning by planning with a learned model},
  author={Schrittwieser, Julian and Hubert, Thomas and Mandhane, Amol and Barekatain, Mohammadamin and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:2104.06294},
  year={2021}
}

@article{bouthillier2021accounting,
  title={Accounting for variance in machine learning benchmarks},
  author={Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

@inproceedings{li2019towards, 
 title={Towards explaining the regularization effect of initial large learning rate in training neural networks}, 
 author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu}, 
 booktitle={Advances in Neural Information Processing Systems}, 
 pages={11674--11685}, 
 year={2019} 
 }

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@article{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  journal={ICML},
  year={2019}
}

@book{hastie2015sparsity,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
year = {2015},
isbn = {1498712169},
publisher = {Chapman &amp; Hall/CRC},
}

@book{bellman1957dynamic,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
}


@inproceedings{boyan1999least,
  title={Least-squares temporal difference learning},
  author={Boyan, Justin A},
  booktitle={ICML},
  pages={49--56},
  year={1999},
  organization={Citeseer}
}

@article{igl2020impact,
  title={The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning},
  author={Igl, Maximilian and Farquhar, Gregory and Luketina, Jelena and Boehmer, Wendelin and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2006.05826},
  year={2020}
}

@article{xu2007kernel,
  title={Kernel-based least squares policy iteration for reinforcement learning},
  author={Xu, Xin and Hu, Dewen and Lu, Xicheng},
  journal={IEEE Transactions on Neural Networks},
  volume={18},
  number={4},
  pages={973--992},
  year={2007},
  publisher={IEEE}
}

@article{xu2005kernel,
  title={Kernel least-squares temporal difference learning},
  author={Xu, Xin and Xie, Tao and Hu, Dewen and Lu, Xicheng},
  journal={International Journal of Information Technology},
  volume={11},
  number={9},
  pages={54--63},
  year={2005}
}

@article{dabney2020value,
  title={The Value-Improvement Path: Towards Better Representations for Reinforcement Learning},
  author={Dabney, Will and Barreto, Andr{\'e} and Rowland, Mark and Dadashi, Robert and Quan, John and Bellemare, Marc G and Silver, David},
  journal={arXiv preprint arXiv:2006.02243},
  year={2020}
}

@techreport{townsend2016differentiating,
  title={Differentiating the singular value decomposition},
  author={Townsend, James},
  year={2016},
  institution={Technical Report 2016, https://j-towns. github. io/papers/svd-derivative~â€¦}
}

@Inbook{Hlawka1991dirichlet,
author="Hlawka, Edmund
and Taschner, Rudolf
and Schoi{\ss}engeier, Johannes",
title="The Dirichlet Approximation Theorem",
bookTitle="Geometric and Analytic Number Theory",
year="1991",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--18",
isbn="978-3-642-75306-0",
doi="10.1007/978-3-642-75306-0_1",
url="https://doi.org/10.1007/978-3-642-75306-0_1"
}



@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@book{duffy2015green,
  title={Green's functions with applications},
  author={Duffy, Dean G},
  year={2015},
  publisher={CRC Press}
}

@article{jaderberg2016reinforcement,
  title={Reinforcement learning with unsupervised auxiliary tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1611.05397},
  year={2016}
}

@article{dabney2017distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1710.10044},
  year={2017}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@article{yang2019harnessing,
  title={Harnessing structures for value-based planning and reinforcement learning},
  author={Yang, Yuzhe and Zhang, Guo and Xu, Zhi and Katabi, Dina},
  journal={arXiv preprint arXiv:1909.12255},
  year={2019}
}

@article{fedus2020catastrophic,
  title={On Catastrophic Interference in Atari 2600 Games},
  author={Fedus, William and Ghosh, Dibya and Martin, John D and Bellemare, Marc G and Bengio, Yoshua and Larochelle, Hugo},
  journal={arXiv preprint arXiv:2002.12499},
  year={2020}
}

@article{paine2020hyperparameter,
  title={Hyperparameter selection for offline reinforcement learning},
  author={Paine, Tom Le and Paduraru, Cosmin and Michi, Andrea and Gulcehre, Caglar and Zolna, Konrad and Novikov, Alexander and Wang, Ziyu and de Freitas, Nando},
  journal={arXiv preprint arXiv:2007.09055},
  year={2020}
}

@article{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang M and Voloshin, Cameron and Yue, Yisong},
  journal={arXiv preprint arXiv:1903.08738},
  year={2019}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}

@inproceedings{luo2020i4r,
  title     = {I4R: Promoting Deep Reinforcement Learning by the Indicator for Expressive Representations},
  author    = {Luo, Xufang and Meng, Qi and He, Di and Chen, Wei and Wang, Yunhong},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  editor    = {Christian Bessiere},	
  pages     = {2669--2675},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/370},
  url       = {https://doi.org/10.24963/ijcai.2020/370},
}


@inproceedings{
Sanyal2020Stable,
title={Stable Rank Normalization for Improved Generalization in Neural Networks and GANs},
author={Amartya Sanyal and Philip H. Torr and Puneet K. Dokania},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1enKkrFDB}
}

@book{puterman1994markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons, Inc.}
}

@article{huber1964robust,
  title={Robust estimation of a location parameter},
  author={Huber, PJ},
  journal={Ann. Math. Stat.},
  year={1964}
}


@article{mnih2013playing,
  title={{Playing Atari with deep reinforcement learning}},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv:1312.5602},
  year={2013}
}





@inproceedings{
daskalakis2018training,
title={Training {GAN}s with Optimism},
author={Constantinos Daskalakis and Andrew Ilyas and Vasilis Syrgkanis and Haoyang Zeng},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SJJySbbAZ},
}

@inproceedings{sutton1999PG,
  author    = {Richard S. Sutton and
               David A. McAllester and
               Satinder P. Singh and
               Yishay Mansour},
  editor    = {Sara A. Solla and
               Todd K. Leen and
               Klaus{-}Robert M{\"{u}}ller},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems 12, {[NIPS} Conference,
               Denver, Colorado, USA, November 29 - December 4, 1999]},
}

@PhdThesis{Watkins1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}

@inproceedings{hou2017novel,
  title={A novel ddpg method with prioritized experience replay},
  author={Hou, Yuenan and Liu, Lifeng and Wei, Qing and Xu, Xudong and Chen, Chunlin},
  booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={316--321},
  year={2017},
  organization={IEEE}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{horgan2018distper,
  author    = {Dan Horgan and
               John Quan and
               David Budden and
               Gabriel Barth{-}Maron and
               Matteo Hessel and
               Hado van Hasselt and
               David Silver},
  title     = {Distributed Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1803.00933},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.00933},
  archivePrefix = {arXiv},
  eprint    = {1803.00933},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-00933.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{absention,
author = {Thulasidasan, Sunil and Bhattacharya, Tanmoy and Bilmes, Jeff and Chennupati, Gopinath and Mohd-Yusof, Jamal},
year = {2019},
month = {05},
booktitle = {Proceedings of 35th International Conference on Machine Learning},
title = {Combating Label Noise in Deep Learning Using Abstention}
}

@inproceedings{hassalt10doubleq,
 author = {Hasselt, Hado van},
 title = {Double Q-Learning},
 year = {2010},
 booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2}
}

@article{ODonoghue2017TheUB,
  title={The Uncertainty Bellman Equation and Exploration},
  author={Brendan O'Donoghue and Ian Osband and R{\'e}mi Munos and Volodymyr Mnih},
  journal={ICML},
  year={2018},
  volume={abs/1709.05380}
}

@inproceedings{hazan2019maxent,
title	= {Provably Efficient Maximum Entropy Exploration},
author	= {Elad Hazan and Sham Kakade and Karan Singh and Abby Van Soest},
year	= {2019},
journal = {ICML},
URL	= {https://arxiv.org/pdf/1812.02690.pdf}
}


@TECHREPORT{Tsitsiklis97ananalysis,
    author = {John N. Tsitsiklis and Benjamin Van Roy},
    title = {An analysis of temporal-difference learning with function approximation},
    institution = {IEEE Transactions on Automatic Control},
    year = {1997}
}

@inproceedings{rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{zhang2017deeper,
  title={A deeper look at experience replay},
  author={Zhang, Shangtong and Sutton, Richard S},
  journal={arXiv preprint arXiv:1712.01275},
  year={2017}
}

@inproceedings{liu2018effects,
  title={The effects of memory replay in reinforcement learning},
  author={Liu, Ruishan and Zou, James},
  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  year={2018},
  organization={IEEE}
}

@inproceedings{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10608--10619},
  year={2018}
}

@article{ruhe1975closeness,
  title={On the closeness of eigenvalues and singular values for almost normal matrices},
  author={Ruhe, Axel},
  journal={Linear Algebra and its Applications},
  volume={11},
  number={1},
  pages={87--93},
  year={1975},
  publisher={Elsevier}
}

@article{wu2019behavior,
  title={Behavior Regularized Offline Reinforcement Learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@InProceedings{fujimoto19a,
  title = 	 {Off-Policy Deep Reinforcement Learning without Exploration},
  author = 	 {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}

@incollection{NIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8026--8037},
year = {2019},
}

@inproceedings{tensorflow,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}

@inproceedings{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle={International Conference on Machine Learning},
  pages={3703--3712},
  year={2019},
  organization={PMLR}
}

@inproceedings{
fu2021benchmarks,
title={Benchmarks for Deep Off-Policy Evaluation},
author={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=kWSeGEeHvF8}
}


@inproceedings{gulcehre2020rl,
  title={Rl unplugged: Benchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Tom Le and Colmenarejo, Sergio G{\'o}mez and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh and Mankowitz, Daniel and Paduraru, Cosmin and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@book{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  volume={37},
  year={1994}
}

@inproceedings{perkins2002api,
author = {Perkins, Theodore J. and Precup, Doina},
title = {A Convergent Form of Approximate Policy Iteration},
year = {2002},
series = {NIPSâ€™02}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}

@article{lstd,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={33--57},
  year={1996},
  publisher={Springer}
}

@article{bellemare2013ale,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = may,
pages = {253â€“279},
numpages = {27}
}
  
@article{yu2020gradient,
  title={Gradient Surgery for Multi-Task Learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}

  
@incollection{ntk,
title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
booktitle = {Advances in Neural Information Processing Systems 31},
year = {2018},
url = {}
}

@article{lorraine2019optimizing,
  title={Optimizing Millions of Hyperparameters by Implicit Differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  journal={arXiv preprint arXiv:1911.02590},
  year={2019}
}


@inproceedings{peters2010reps,
author = {Peters, Jan and M\"{u}lling, Katharina and Alt\"{u}n, Yasemin},
title = {Relative Entropy Policy Search},
year = {2010},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1607â€“1612},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAIâ€™10}
}
  


@article{machado18sticky,
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
title = {Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523â€“562},
numpages = {40}
}
  


@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={449--458},
  year={2017},
  organization={JMLR. org}
}


@article{castro18dopamine,
  author    = {Pablo Samuel Castro and
               Subhodeep Moitra and
               Carles Gelada and
               Saurabh Kumar and
               Marc G. Bellemare},
  title     = {Dopamine: {A} {R}esearch {F}ramework for {D}eep {R}einforcement {L}earning},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06110},
  archivePrefix = {arXiv}
}

@article{scherrer15a,
  author  = {Bruno Scherrer and Mohammad Ghavamzadeh and Victor Gabillon and Boris Lesner and Matthieu Geist},
  title   = {Approximate Modified Policy Iteration and its Application to the Game of Tetris},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {49},
  pages   = {1629-1676},
  url     = {http://jmlr.org/papers/v16/scherrer15a.html}
}

@article{Lesner2013TightPB,
  title={Tight Performance Bounds for Approximate Modified Policy Iteration with Non-Stationary Policies},
  author={Boris Lesner and Bruno Scherrer},
  journal={ArXiv},
  year={2013},
  volume={abs/1304.5610}
}

@inproceedings{scherrer_comparison,
author = {Scherrer, Bruno},
title = {Approximate Policy Iteration Schemes: A Comparison},
year = {2014},
publisher = {JMLR.org},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {IIâ€“1314â€“IIâ€“1322},
numpages = {9},
location = {Beijing, China},
series = {ICMLâ€™14}
}
  


@inproceedings{
du2020is,
title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
author={Simon S. Du and Sham M. Kakade and Ruosong Wang and Lin F. Yang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1genAVKPB}
}

@inproceedings{munos2003api,
author = {Munos, R\'{e}mi},
title = {Error Bounds for Approximate Policy Iteration},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {560â€“567},
numpages = {8},
location = {Washington, DC, USA},
series = {ICMLâ€™03}
}
  


@inproceedings{yu2019meta,
  title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019},
  eprint={1910.10897},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1910.10897},
}

@article{kumar19bear,
  author       = {Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
  title        = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  conference   = {NeurIPS 2019},
  year = {2019},
  url          = {http://arxiv.org/abs/1906.00949},
}

@article{farias_fixed_points,
author = {Farias, D. and Roy, B.},
year = {2000},
month = {06},
pages = {589-608},
title = {On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning},
volume = {105},
journal = {Journal of Optimization Theory and Applications},
doi = {10.1023/A:1004641123405}
}

@inproceedings{Krantz2002TheIF,
  title={The Implicit Function Theorem: History, Theory, and Applications},
  author={Steven G. Krantz and Harold R. Parks},
  year={2002}
}

@InProceedings{ahmed19understanding,
  title = 	 {Understanding the Impact of Entropy on Policy Optimization},
  author = 	 {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  publisher = 	 {PMLR},
  year = {2019},
  url = 	 {http://proceedings.mlr.press/v97/ahmed19a.html},
  abstract = 	 {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.}
}

@book{rockafellar-1970a,
  added-at = {2008-03-02T02:12:02.000+0100},
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  biburl = {https://www.bibsonomy.org/bibtex/223aa07ea525f6dd11585fc2037a0daf1/dmartins},
  callnumber = {UniM Maths 516.08 R59},
  description = {robotica-bib},
  interhash = {30830becb0a2c5ebca5946b895d9740a},
  intrahash = {23aa07ea525f6dd11585fc2037a0daf1},
  keywords = {imported},
  notes = {A SRL reference.},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  timestamp = {2008-03-02T02:14:11.000+0100},
  title = {Convex analysis},
  year = 1970
}


@article{Achiam2019TowardsCD,
  title={Towards Characterizing Divergence in Deep Q-Learning},
  author={Joshua Achiam and Ethan Knight and Pieter Abbeel},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.08894}
}

@Article{Tsitsiklis1994,
author="Tsitsiklis, John N.",
title="Asynchronous stochastic approximation and Q-learning",
journal="Machine Learning",
year="1994",
month="Sep",
day="01",
volume="16",
number="3",
pages="185--202",
abstract="We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.",
issn="1573-0565",
doi="10.1007/BF00993306",
url="https://doi.org/10.1007/BF00993306"
}



  @inproceedings{devraj2017zap,
 author = {Devraj, Adithya M. and Meyn, Sean P.},
 title = {Zap Q-Learning},
 year = {2017},
 isbn = {9781510860964},
 publisher = {Curran Associates Inc.},
 address = {Red Hook, NY, USA},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 pages = {2232â€“2241},
 numpages = {10},
 location = {Long Beach, California, USA},
 series = {NIPSâ€™17}
}


@inproceedings{maei09nonlineargtd,
 author = {Maei, Hamid R. and Szepesv\'{a}ri, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S.},
 title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
 year = {2009},
 booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems}
}


@article{Hasselt2018DeepRL,
  title={Deep Reinforcement Learning and the Deadly Triad},
  author={Hado van Hasselt and Yotam Doron and Florian Strub and Matteo Hessel and Nicolas Sonnerat and Joseph Modayil},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.02648}
}

@inproceedings{Kolter2011TheFP,
  title={The Fixed Points of Off-Policy TD},
  author={J. Zico Kolter},
  booktitle={NIPS},
  year={2011}
}

@inproceedings{du2019distributioncheck,
author = {Du, Simon and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
year = {2019},
month = {06},
booktitle={NeurIPS},
title = {Provably Efficient $Q$-learning with Function Approximation via Distribution Shift Error Checking Oracle}
}

@article{sutton16emphatic,
author = {Sutton, Richard S. and Mahmood, A. Rupam and White, Martha},
title = {An Emphatic Approach to the Problem of Off-Policy Temporal-Difference Learning},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2603â€“2631},
numpages = {29},
keywords = {function approximation, temporal-difference learning, off-policy learning, convergence, stability}
}

@InProceedings{fu19diagnosing,
  title = 	 {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  author = 	 {Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  year = {2019},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  publisher = 	 {PMLR},
}


@phdthesis{konda_ac,
author = {Konda, Vijaymohan and Tsitsiklis, John N.},
title = {Actor-Critic Algorithms},
year = {2002},
publisher = {Massachusetts Institute of Technology},
address = {USA},
note = {AAI0804543}
}


@article{schaul2019ray,
  author    = {Tom Schaul and
               Diana Borsa and
               Joseph Modayil and
               Razvan Pascanu},
  title     = {Ray Interference: a Source of Plateaus in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1904.11455},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.11455},
  archivePrefix = {arXiv},
  eprint    = {1904.11455},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-11455},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout


@inproceedings{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2010}
}

@article{tsitsiklis1994asynchronous,
  title={Asynchronous stochastic approximation and Q-learning},
  author={Tsitsiklis, John N},
  journal={Machine learning},
  volume={16},
  number={3},
  pages={185--202},
  year={1994},
  publisher={Springer}
}

@inproceedings{
yaz2018the,
title={The Unusual Effectiveness of Averaging in {GAN} Training},
author={Yasin Yaz{\i}c{\i} and Chuan-Sheng Foo and Stefan Winkler and Kim-Hui Yap and Georgios Piliouras and Vijay Chandrasekhar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJgw_sRqFQ},
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@inproceedings{Kakade2002,
author = {Kakade, Sham and Langford, John},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
year = {2002}
}

@inproceedings{Maei2010,
author = {Maei, Hamid Reza and Szepesv{\'{a}}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Toward off-policy learning control with function approximation}},
year = {2010}
}

@inproceedings{Baird1995,
annote = {Residual gradient = differentiating through target},
author = {Baird, Leemon},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Residual Algorithms : Reinforcement Learning with Function Approximation}},
year = {1995}
}

@article{Mnih2015,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}

@article{Lillicrap2015,
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Continuous control with deep reinforcement learning}},
year = {2015}
}

@inproceedings{Schulman2015,
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Trust Region Policy Optimization}},
year = {2015}
}

@inproceedings{Precup2001,
author = {Precup, Doina and Sutton, Richard S. and Dasgupta, Sanjoy},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Off-policy temporal-difference learning with function approximation}},
year = {2001}
}

@article{Schulman2017,
  author    = {John Schulman and
               Pieter Abbeel and
               Xi Chen},
  title     = {Equivalence Between Policy Gradients and Soft Q-Learning},
  journal   = {ArXiv Preprint},
  volume    = {abs/1704.06440},
  year      = {2017},
}

@article{Gu2016,
  author    = {Shixiang Gu and
               Ethan Holly and
               Timothy Lillicrap and
               Sergey Levine},
  title     = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  journal   = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2017},
}

@article{Watkins92,
  author    = {Christopher J.C.H. Watkins and Peter Dayan},
  title     = {Q-learning},
  journal   = {Machine Learning},
  year      = {1992},
  volume    = {8},
  pages     = {279-292},
  issue     = {3},
}

@incollection{Kakade2001,
title = {A Natural Policy Gradient},
author = {Sham M. Kakade},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2001},
}

@incollection{Munos2016,
title = {Safe and Efficient Off-policy Reinforcement Learning},
author = {Remi Munos and Thomas Stepleton and Anna Harutyunyan and Marc G. Bellemare},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2016},
}

@inproceedings{Sutton09a,
title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
author = {Sutton, Richard S. and Hamid Reza Maei and Csaba Szepesv\'{a}ri},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2009},
}

@inproceedings{Sutton09b,
title = {Fast Gradient-Descent Methods for Temporal-Difference Learning
with Linear Function Approximation},
author = {
Richard S. Sutton and Hamid Reza Maei and Doina Precup and Shalabh Bhatnagar and David Silver and Csaba Szepesv\'{a}ri and
Eric Wiewiora},
booktitle = {International Conference on Machine Learning (ICML)},
year = {2009},
}

@article{Bertsekas96,
title = {Neuro-Dynamic Programming},
author = {Bertsekas, Dimitri P., and Tsitsiklis, John N.},
journal = {Athena Scientific},
year = {1996},
}


@inproceedings{Haarnoja2017,
author = {Tuomas Haarnoja and
               Haoran Tang and
               Pieter Abbeel and
               Sergey Levine},
booktitle = {International Conference on Machine Learning (ICML)},
title = {Reinforcement Learning with Deep Energy-Based Policies},
year = {2017}
}

@inproceedings{Hausknecht2016,
author = {Matthew Hausknecht and Peter Stone},
booktitle = {Deep Reinforcement Learning: Frontiers and Challenges Workshop, IJCAI},
title = {On-Policy vs. Off-Policy Updates for Deep Reinforcement Learning},
year = {2016}
}

@inproceedings{Farrell95,
author = {Jay A. Farrell and T. Berger},
booktitle = {American Control Conference},
title = {On the effects of the training sample density in passive learning control},
year = {1995}
}

@inproceedings{Boyan94,
author = {Justin A. Boyan and Andrew W. Moore},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
title = {Generalization in Reinforcement Learning:
Safely Approximating the Value Function},
year = {1994}
}

@article{Haarnoja18,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
}

@inproceedings{kalashnikov18,
  author    = {Dmitry Kalashnikov and
               Alex Irpan and
               Peter Pastor and
               Julian Ibarz and
               Alexander Herzog and
               Eric Jang and
               Deirdre Quillen and
               Ethan Holly and
               Mrinal Kalakrishnan and
               Vincent Vanhoucke and
               Sergey Levine},
  title     = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  booktitle = {CoRL},
  year      = {2018}
}

@article{Ernst05,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={503--556},
  year={2005}
}

@book{suttonrlbook,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  edition = {Second}
}

@inproceedings{Dai2018,
  title={Sbeed: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1133--1142},
  year={2018}
}

@article{VanHesselt2018,
  title={Deep Reinforcement Learning and the Deadly Triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@inproceedings{Tsitsiklis1997,
  title={Analysis of temporal-diffference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={1075--1081},
  year={1997}
}

@article{Hallak2017,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  journal={arXiv preprint arXiv:1702.07121},
  year={2017}
}

@article{Sutton2016,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}

@article{henderson2017deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal={arXiv preprint arXiv:1709.06560},
  year={2017}
}

@article{dalal2017finite,
  title={Finite sample analyses for TD (0) with function approximation},
  author={Dalal, Gal and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Thoppe, Gugan and Mannor, Shie},
  journal={arXiv preprint arXiv:1704.01161},
  year={2017}
}

@inproceedings{bhatnagar2009convergent,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation},
  author={Maei, Hamid R, and Szepesv{\'a}ri, Csaba, and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S },
  booktitle={Advances in Neural Information Processing Systems},
  pages={1204--1212},
  year={2009}
}

@inproceedings{Riedmiller2005,
  title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
  author={Riedmiller, Martin},
  booktitle={European Conference on Machine Learning},
  pages={317--328},
  year={2005},
  organization={Springer}
}

@article{Watkins1992,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{Gu2017,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  journal={arXiv preprint arXiv:2007.06700},
  year={2020}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@misc{approxstack,
  Author = {Robert Johnson},
  Title = {Approximate Irrational Numbers by Rational Numbers},
  Year = {2016},
  url = {https://math.stackexchange.com/questions/1829743/},
}

@article{Schaul2015,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@book{Shalev2014,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@incollection{NIPS2017_6913,
title = {Is the Bellman residual a bad proxy?},
author = {Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {3205--3214},
year = {2017},
}

@inproceedings{Pritzel2017,
  title={Neural Episodic Control},
  author={Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adri{\`a} Puigdom{\`e}nech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2827--2836},
  year={2017}
}

@InProceedings{pmlr-v80-fujimoto18a,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {1587--1596},
  year = 	 {2018},
}

@inproceedings{Gu2017ipg,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={3846--3855},
  year={2017}
}

@inproceedings{maillard2010finite,
  title={Finite-sample analysis of Bellman residual minimization},
  author={Maillard, Odalric-Ambrym and Munos, R{\'e}mi and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
  booktitle={Asian Conference on Machine Learning (ACML)},
  pages={299--314},
  year={2010}
}

@inproceedings{munos2005error,
  title={Error bounds for approximate value iteration},
  author={Munos, R{\'e}mi},
  booktitle={AAAI Conference on Artificial intelligence (AAAI)},
  pages={1006--1011},
  year={2005},
  organization={AAAI Press}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@article{zhang2018deeper,
  author    = {Shangtong Zhang and
               Richard S. Sutton},
  title     = {A Deeper Look at Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1712.01275},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01275},
  archivePrefix = {arXiv},
  eprint    = {1712.01275},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01275},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{antos2007fitted,
 author = {Antos, Andr\'{a}s and Munos, R{\'e}mi and Szepesv\'{a}ri, Csaba},
 title = {Fitted Q-iteration in Continuous Action-space MDPs},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 series = {NIPS'07},
 year = {2007},
 location = {Vancouver, British Columbia, Canada},
} 

@phdthesis{de2002alp,
  title={The linear programming approach to approximate dynamic programming: Theory and application},
  author={De Farias, Daniela Pucci},
  year={2002}
}

@article{antos2008concentrability,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@article{metelli2018nips,
  author    = {Alberto Maria Metelli and
               Matteo Papini and
               Francesco Faccio and
               Marcello Restelli},
  title     = {Policy Optimization via Importance Sampling},
  journal   = {CoRR},
  volume    = {abs/1809.06098},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.06098},
  archivePrefix = {arXiv},
  eprint    = {1809.06098},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-06098},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {214--223},
  year = 	 {2017},
}

@techreport{haarnoja2018sacapps,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{hazan2018,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham M and Singh, Karan and Van Soest, Abby},
  journal={arXiv preprint arXiv:1812.02690},
  year={2018}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{scherrer2010residual,
  title={Should one compute the temporal difference fix point or minimize the Bellman Residual? The unified oblique projection view},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={959--966},
  year={2010},
}

@inproceedings{tosatto2017boosted,
  title={Boosted fitted q-iteration},
  author={Tosatto, Samuele and Pirotta, Matteo and D'Eramo, Carlo and Restelli, Marcello},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={3434--3443},
  year={2017},
  organization={JMLR. org}
}

@article{lin1992replay,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={1054--1062},
  year={2016}
}

@article{sriperum2009ipms,
  author    = {Bharath K. Sriperumbudur and
               Arthur Gretton and
               Kenji Fukumizu and
               Gert R. G. Lanckriet and
               Bernhard Sch{\"{o}}lkopf},
  title     = {A note on integral probability metrics and {\textdollar}{\textbackslash}phi{\textdollar}-divergences},
  journal   = {CoRR},
  volume    = {abs/0901.2698},
  year      = {2009},
  url       = {http://arxiv.org/abs/0901.2698},
  archivePrefix = {arXiv},
  eprint    = {0901.2698},
  timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-0901-2698},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kumar2020discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2003.07305},
  year={2020}
}

@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7413--7424},
  year={2019}
}

@inproceedings{yang2020theoretical,
  title={A theoretical analysis of deep Q-learning},
  author={Yang, Zhuoran and Xie, Yuchen and Wang, Zhaoran},
  booktitle={Learning for Dynamics and Control},
  pages={486--489},
  year={2020},
  organization={PMLR}
}

@article{cai2019neural,
  title={Neural Temporal-Difference and Q-Learning Provably Converge to Global Optima},
  author={Cai, Qi and Yang, Zhuoran and Lee, Jason D and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1905.10027},
  year={2019}
}

@article{mahadevan2007proto,
  title={Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes},
  author={Mahadevan, Sridhar and Maggioni, Mauro},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Oct},
  pages={2169--2231},
  year={2007}
}

@article{wu2018laplacian,
  title={The Laplacian in RL: Learning representations with efficient approximations},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1810.04586},
  year={2018}
}

@article{machado2017eigenoption,
  title={Eigenoption discovery through the deep successor representation},
  author={Machado, Marlos C and Rosenbaum, Clemens and Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald and Campbell, Murray},
  journal={arXiv preprint arXiv:1710.11089},
  year={2017}
}

@article{van2019use,
  title={When to use parametric models in reinforcement learning?},
  author={van Hasselt, Hado and Hessel, Matteo and Aslanides, John},
  journal={arXiv preprint arXiv:1906.05243},
  year={2019}
}

@article{tian2021understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2102.06810},
  year={2021}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}

@article{chen2020exploring,
  title={Exploring Simple Siamese Representation Learning},
  author={Chen, Xinlei and He, Kaiming},
  journal={arXiv preprint arXiv:2011.10566},
  year={2020}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent: A new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and others},
  journal={arXiv preprint arXiv:2006.07733},
  year={2020}
}

@article{fakoor2021continuous,
  title={Continuous Doubly Constrained Batch Reinforcement Learning},
  author={Fakoor, Rasool and Mueller, Jonas and Chaudhari, Pratik and Smola, Alexander J},
  journal={arXiv preprint arXiv:2102.09225},
  year={2021}
}

@article{kaiser2019model,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@article{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:2002.09277},
  year={2020}
}

@inproceedings{lyle2021effect,
  title={On The Effect of Auxiliary Tasks on Representation Dynamics},
  author={Lyle, Clare and Rowland, Mark and Ostrovski, Georg and Dabney, Will},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1--9},
  year={2021},
  organization={PMLR}
}

@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason and Liu, Qiang and Ma, Tengyu},
  year={2019}
}

@article{sedghi2018singular,
  title={The singular values of convolutional layers},
  author={Sedghi, Hanie and Gupta, Vineet and Long, Philip M},
  journal={arXiv preprint arXiv:1805.10408},
  year={2018}
}

@article{zhang2020can,
  title={Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory},
  author={Zhang, Yufeng and Cai, Qi and Yang, Zhuoran and Chen, Yongxin and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2006.04761},
  year={2020}
}

@article{xu2019finite,
  title={A finite-time analysis of Q-learning with neural network function approximation},
  author={Xu, Pan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.04511},
  year={2019}
}

@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}

@article{ghosh2020representations,
  title={Representations for Stable Off-Policy Reinforcement Learning},
  author={Ghosh, Dibya and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2007.05520},
  year={2020}
}

@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2002.05715},
  year={2020}
}

@inproceedings{agarwal2019optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{kumar2020conservative,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@article{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal={arXiv preprint arXiv:1802.06509},
  year={2018}
}

@inproceedings{boyan1999least,
  title={Least-squares temporal difference learning},
  author={Boyan, Justin A},
  booktitle={ICML},
  pages={49--56},
  year={1999},
  organization={Citeseer}
}

@inproceedings{melo2008analysis,
  title={An analysis of reinforcement learning with function approximation},
  author={Melo, Francisco S and Meyn, Sean P and Ribeiro, M Isabel},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={664--671},
  year={2008}
}

@inproceedings{devraj2017zap,
  title={Zap Q-learning},
  author={Devraj, Adithya M and Meyn, Sean},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2235--2244},
  year={2017}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{bengio2020interference,
  title={Interference and Generalization in Temporal Difference Learning},
  author={Bengio, Emmanuel and Pineau, Joelle and Precup, Doina},
  journal={arXiv preprint arXiv:2003.06350},
  year={2020}
}

@inproceedings{precup2001offpol,
  title={Off-Policy Temporal Difference Learning with Function Approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={417--424},
  year={2001},
}

@article{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1806.06923},
  year={2018}
}

@inproceedings{hausknecht2016policy,
  title={On-policy vs. off-policy updates for deep reinforcement learning},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={Deep Reinforcement Learning: Frontiers and Challenges, IJCAI},
  year={2016}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{martha2018sparse,
  author    = {Vincent Liu and
               Raksha Kumaraswamy and
               Lei Le and
               Martha White},
  title     = {The Utility of Sparse Representations for Control in Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1811.06626},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.06626},
  archivePrefix = {arXiv},
  eprint    = {1811.06626},
  timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-06626},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@article{shani2005recommender,
  title={An MDP-based recommender system},
  author={Shani, Guy and Heckerman, David and Brafman, Ronen I},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Sep},
  pages={1265--1295},
  year={2005}
}

@inproceedings{szepesvari1998asymptotic,
  title={The asymptotic convergence-rate of Q-learning},
  author={Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1064--1070},
  year={1998}
}

@inproceedings{ijcai2020-370,
  title     = {I4R: Promoting Deep Reinforcement Learning by the Indicator for Expressive Representations},
  author    = {Luo, Xufang and Meng, Qi and He, Di and Chen, Wei and Wang, Yunhong},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {2669--2675},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/370},
  url       = {https://doi.org/10.24963/ijcai.2020/370},
}

@article{yoshida2017spectral,
  title={Spectral norm regularization for improving the generalizability of deep learning},
  author={Yoshida, Yuichi and Miyato, Takeru},
  journal={arXiv preprint arXiv:1705.10941},
  year={2017}
}

@inproceedings{Parr2008AnAO,
  title={An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning},
  author={Ronald Parr and Lihong Li and Gavin Taylor and Christopher Painter-Wakefield and Michael L. Littman},
  booktitle={ICML '08},
  year={2008}
}


@book{koller_friedman,
 author = {Koller, D. and Friedman, N.},
 title = {Probabilistic Graphical Models: Principles and Techniques},
 year = {2009},
 isbn = {0262013193, 9780262013192},
 publisher = {The MIT Press},
} 

@inproceedings{chebotar2019closing,
  title={Closing the sim-to-real loop: Adapting simulation randomization with real world experience},
  author={Chebotar, Yevgen and Handa, Ankur and Makoviychuk, Viktor and Macklin, Miles and Issac, Jan and Ratliff, Nathan and Fox, Dieter},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8973--8979},
  year={2019},
  organization={IEEE}
}

@article{tan2018sim,
  title={Sim-to-real: Learning agile locomotion for quadruped robots},
  author={Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
  journal={arXiv preprint arXiv:1804.10332},
  year={2018}
}

@inproceedings{sadeghi2016cad2rl,
  title={{CAD2RL}: Real single-image flight without a single real image},
  author={Sadeghi, Fereshteh and Levine, Sergey},
  booktitle={Robotics: Science and Systems},
  year={2017}
}

@inproceedings{gae,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {J. Schulman and P. Moritz and S. Levine and M. Jordan and P. Abbeel},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2016
}

@misc{kumar_blog,
title = {Data-Driven Deep Reinforcement Learning},
author = {Aviral Kumar},
note = {{BAIR} Blog},
year = {2019},
howpublished  = {\url{https://bair.berkeley.edu/blog/2019/12/05/bear/}}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  pages={1--9},
  year={2013}
}

@article{luo2019learning,
  title={Learning self-correctable policies and value functions from demonstrations with negative sampling},
  author={Luo, Yuping and Xu, Huazhe and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.05634},
  year={2019}
}

@inproceedings{mulayoff2020unique,
  title={Unique Properties of Flat Minima in Deep Networks},
  author={Mulayoff, Rotem and Michaeli, Tomer},
  booktitle={International Conference on Machine Learning},
  pages={7108--7118},
  year={2020},
  organization={PMLR}
}

@article{van2018deep,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@article{damian2021label,
  title={Label Noise SGD Provably Prefers Flat Global Minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason},
  journal={arXiv preprint arXiv:2106.06530},
  year={2021}
}

@article{mahmood2015emphatic,
  title={Emphatic temporal-difference learning},
  author={Mahmood, A Rupam and Yu, Huizhen and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1507.01569},
  year={2015}
}

@article{chatterji2019intriguing,
  title={The intriguing role of module criticality in the generalization of deep networks},
  author={Chatterji, Niladri S and Neyshabur, Behnam and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1912.00528},
  year={2019}
}

@article{bjorck2021towards,
  title={Towards Deeper Deep Reinforcement Learning},
  author={Bjorck, Johan and Gomes, Carla P and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:2106.01151},
  year={2021}
}

@article{ota2021training,
  title={Training Larger Networks for Deep Reinforcement Learning},
  author={Ota, Kei and Jha, Devesh K and Kanezaki, Asako},
  journal={arXiv preprint arXiv:2102.07920},
  year={2021}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{sinha2020d2rl,
  title={D2rl: Deep dense architectures in reinforcement learning},
  author={Sinha, Samarth and Bharadhwaj, Homanga and Srinivas, Aravind and Garg, Animesh},
  journal={arXiv preprint arXiv:2010.09163},
  year={2020}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Advances in neural information processing systems},
  pages={1531--1538},
  year={2002}
}


@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={503--556},
  year={2005}
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@InProceedings{thomas,
  title = 	 {Bias in Natural Actor-Critic Algorithms},
  author = 	 {Philip Thomas},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2014},
  month = 	 {22--24 Jun},
}

@inproceedings{toussaint06,
 author = {Toussaint, M. and Storkey, A.},
 title = {Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2006},
} 

@INPROCEEDINGS{attias,
    author = {H. Attias},
    title = {Planning by Probabilistic Inference},
    booktitle = {Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics},
    year = {2003}
}

@article{hafner2011reinforcement,
  title={Reinforcement learning in feedback control},
  author={Hafner, Roland and Riedmiller, Martin},
  journal={Machine learning},
  volume={84},
  number={1-2},
  pages={137--169},
  year={2011},
  publisher={Springer}
}

@article{tesauro1994td,
  title={{TD-Gammon}, a self-teaching backgammon program, achieves master-level play},
  author={Tesauro, Gerald},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={215--219},
  year={1994},
  publisher={MIT Press}
}

@InProceedings{ebrl,
  title = 	 {Actor-Critic Reinforcement Learning with Energy-Based Policies},
  author = 	 {N. Heess and D. Silver and Y. W. Teh},
  booktitle = 	 {European Workshop on Reinforcement Learning (EWRL)},
  year = 	 {2013},
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={627--635},
  year={2011}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={661--668},
  year={2010}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning (ICML)},
  volume={2},
  year={2002}
}

@inproceedings{ep,
 author = {Minka, T. P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Uncertainty in Artificial Intelligence (UAI)},
 year = {2001},
}

@inproceedings{mpo,
title = {Maximum a Posteriori Policy Optimisation},
author = {A. Abdolmaleki and J. T. Springenberg and Y. Tassa and R. Munos and N. Heess and M. Riedmiller},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2018
}

@article{williams,
 author = {Williams, R. J.},
 title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
 journal = {Machine Learning},
 issue_date = {May 1992},
 volume = {8},
 number = {3-4},
 month = may,
 year = {1992},
 pages = {229--256}
} 

@article{WilliamsPeng91,
  author = {Williams, R. J. and Peng, J.},
  journal = {Connection Science},
  number = 3,
  pages = {241-268},
  title = {Function optimization using connectionist reinforcement learning
	algorithms},
  volume = 3,
  year = 1991
}

@book{sb-irl-98,
 author = {Sutton, R. S. and Barto, A. G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{suttonadp,
    author = {R. S. Sutton},
    title = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {1990},
}

@inproceedings{pgq,
author = {O'Donoghue, B. and Munos, R. and Kavukcuoglu, K. and Mnih, V.},
year = {2017},
title = {PGQ: Combining policy gradient and Q-learning},
booktitle = {International Conference on Learning Representations (ICLR)}
}

@article{sallans,
 author = {Sallans, B. and Hinton, G. E.},
 title = {Reinforcement Learning with Factored States and Actions},
 journal = {Journal of Machine Learning Research},
 volume = {5},
 month = dec,
 year = {2004}
 },

@article{KLMSurvey,
    author = "L. P. Kaelbling and M. L. Littman and A. P. Moore",
    title = "Reinforcement Learning: A Survey",
    journal = "Journal of Artificial Intelligence Research",
    volume = "4",
    pages = "237-285",
    year = "1996",
url={http://people.csail.mit.edu/lpk/papers/rl-survey.ps}}

@inproceedings{todorov_kalman_duality, 
author={E. Todorov}, 
booktitle={Conference on Decision and Control (CDC)},
title={General duality between optimal control and estimation}, 
year={2008},
}

@inproceedings{covariant,
author = {J. A. Bagnell and J. Schneider},
title = {Covariant Policy Search},
booktitle = {International Joint Conference on Artifical Intelligence (IJCAI)},
year = {2003},
}

@InProceedings{reps,
  author =       "Peters, J. and  M{\"u}lling, K. and Alt{\"u}n, Y.",
  title =        "Relative Entropy Policy Search",
  booktitle =    "AAAI Conference on Artificial Intelligence (AAAI)",
  year =         "2010",
}

@inproceedings{mfgps,
title = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
author = {Levine, S. and Abbeel, P.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{lmdp_policy,
  author    = {E. Todorov},
  title     = {Policy gradients in linearly-solvable MDPs},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year      = {2010},
}

@inproceedings{ldmp_irl,
 author = {Dvijotham, K. and Todorov, E.},
 title = {Inverse Optimal Control with Linearly-solvable MDPs},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2010},
} 

@inproceedings{bear,
title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
author = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
booktitle = {Neural Information Processing Systems (NeurIPS)},
year = {2019}
}

@article{lerer2016learning,
  title={Learning physical intuition of block towers by example},
  author={Lerer, Adam and Gross, Sam and Fergus, Rob},
  journal={arXiv preprint arXiv:1603.01312},
  year={2016}
}

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@inproceedings{finn2017deep,
  title={Deep visual foresight for planning robot motion},
  author={Finn, Chelsea and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2786--2793},
  year={2017},
  organization={IEEE}
}

@inproceedings{battaglia2016interaction,
  title={Interaction networks for learning about objects, relations and physics},
  author={Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo Jimenez and others},
  booktitle={Advances in neural information processing systems},
  pages={4502--4510},
  year={2016}
}

@inproceedings{sagawa2019distributionally,
  title={Distributionally Robust Neural Networks},
  author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{sinha2017certifying,
  title={Certifying some distributional robustness with principled adversarial training},
  author={Sinha, Aman and Namkoong, Hongseok and Duchi, John},
  journal={arXiv preprint arXiv:1710.10571},
  year={2017}
}

@inproceedings{kingma2014semi,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Durk P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  booktitle={Advances in neural information processing systems},
  pages={3581--3589},
  year={2014}
}

@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}

@article{scholkopf2019causality,
  title={Causality for Machine Learning},
  author={Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1911.10500},
  year={2019}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{pi2,
  title = {Learning Policy Improvements with Path Integrals},
  author = {Theodorou, E. A. and Buchli, J. and Schaal, S.},
  booktitle = {International Conference on  Artificial Intelligence and Statistics (AISTATS 2010)},
  year = {2010},
}

@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2015},
}

@phdthesis{levine_thesis,
author = {Levine, S.},
title = {Motor skill learning with local trajectory methods},
school = {Stanford University},
year = {2014},
}

@phdthesis{ziebart_thesis,
author = {Ziebart, B.},
title = {Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
school = {Carnegie Mellon University},
year = {2010},
}

@article{kappen_oc,
  author    = {H. J. Kappen},
  title     = {Optimal control theory and the linear bellman equation},
  journal   = {Inference and Learning in Dynamic Models},
  year      = {2011},
  pages     = {363-387},
}

@inproceedings{heess2015learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2944--2952},
  year={2015}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1352--1361},
  year={2017},
  organization={JMLR. org}
}

@article{kappen_pgm,
  author    = {H. J. Kappen and
               V. G{\'o}mez and
               M. Opper},
  title     = {Optimal control as a graphical model inference problem},
  journal   = {Machine Learning},
  volume    = {87},
  number    = {2},
  year      = {2012},
  pages     = {159-182},
}

@inproceedings{rawlik_soc,
  author    = {K. Rawlik and
               M. Toussaint and
               S. Vijayakumar},
  title     = {On Stochastic Optimal Control and Reinforcement Learning
               by Approximate Inference},
  year = {2013},
  booktitle = {Robotics: Science and Systems (RSS)},
}

@inproceedings{toussaint_soc,
  author    = {M. Toussaint},
  title     = {Robot trajectory optimization using approximate inference},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2009},
}

@inproceedings{toussaint_pgm,
  title={Hierarchical POMDP Controller Optimization by Likelihood Maximization},
  author={Toussaint, M. and Charlin, L. and Poupart, P.},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  volume={24},
  pages={562--570},
  year={2008}
}

@article{kalman_filter,
  author={R. Kalman},
  title={A new approach to linear filtering and prediction problems},
  journal={ASME Transactions journal of basic engineering},
  volume={82},
  number={1},
  pages={35-45},
  year={1960}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{todorov_lmdp,
  author    = {E. Todorov},
  title     = {Linearly-solvable Markov decision problems},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2006},
}

@inproceedings{rwr,
 author = {Peters, J. and Schaal, S.},
 title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2007},
} 


@inproceedings{neumann,
  author    = {G. Neumann},
  title     = {Variational Inference for Policy Search in changing situations},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2011},
}

@inproceedings{levine_vgps,
  author    = {S. Levine and V. Koltun},
  title     = {Variational policy search via trajectory optimization},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2013},
}

@inproceedings{em_policy_search,
  title = {Efficient Sample Reuse in EM-Based Policy Search},
  author = {Hachiya, H. and Peters, J. and Sugiyama, M.},
  booktitle = {European Conference on Machine Learning (ECML)},
  year = {2009},
}

@article{vmp,
    title={Variational message passing},
    author={Winn, J. and Bishop, C.},
    journal={Journal of Machine Learning Research},
    volume={6},
    year={2005},
    pages={661-694}
}

@inproceedings{haarnoja,
  title={Reinforcement Learning with Deep Energy-Based Policies},
  author={Haarnoja, T. and Tang, H. and Abbeel, P. and Levine, S.},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@inproceedings{sac,
title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
author  = {T. Haarnoja and A. Zhou and P. Abbeel and S. Levine},
year  = {2018},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1801.01290.pdf}
}

@inproceedings{d4rl,
title = {D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
author  = {J. Fu and A. Kumar and O. Nachum and G. Tucker and S. Levine},
year  = {2020},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/2004.07219}
}

@inproceedings{pcl,
title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
author  = {O. Nachum and M. Norouzi and K. Xu and D. Schuurmans},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1702.08892.pdf}
}

@inproceedings{gps,
 author = {Levine, S. and Koltun, V.},
 title = {Guided Policy Search},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2013},
} 

@inproceedings{maxentirl,
 author = {Ziebart, B. D. and Maas, A. and Bagnell, J. A. and Dey, A. K.},
 title = {Maximum Entropy Inverse Reinforcement Learning},
 booktitle = {International Conference on Artificial Intelligence (AAAI)},
 year = {2008},
} 

@inproceedings{haarnoja_composable,
author = {Haarnjoa, T. and Pong, V. and Zhou, A. and Dalal, M. and Abbeel, P. and Levine, S.},
title = {Composable Deep Reinforcement Learning for Robotic Manipulation},
booktitle = {International Conference on Robotics and Automation (ICRA)},
year = {2018},
}

@article{trust_pcl,
  author    = {O. Nachum and
               M. Norouzi and
               K. Xu and
               D. Schuurmans},
  title     = {Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1707.01891},
  year      = {2017},
}

@inproceedings{gpirl,
 author = {Levine, S. and Popovi\'{c}, Z. and Koltun, V.},
 title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},
 booktitle = {Neural Information Processing Systems (NIPS)},
 year = {2011},
}

@inproceedings{localioc,
    author = {S. Levine and V. Koltun},
    title = {Continuous Inverse Optimal Control with Locally Optimal Examples},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2012},
}

@inproceedings{legibility,
  author    = {A. D. Dragan and
               K. C. T. Lee and
               S. S. Srinivasa},
  title     = {Legibility and predictability of robot motion},
  booktitle = {International Conference on Human-Robot Interaction (HRI)},
  year      = {2013},
}

@inproceedings{maxentdeepirl,
author = {M. Wulfmeier and
P. Ondruska and
I. Posner},
title = {Maximum Entropy Deep Inverse Reinforcement Learning},
Booktitle = {Neural Information Processing Systems Conference, Deep Reinforcement Learning Workshop},
year = {2015},
}

@inproceedings{maxcausalent,
  author    = {B. D. Ziebart and
               J. A. Bagnell and
               A. K. Dey},
  title     = {Modeling Interaction via the Principle of Maximum Causal Entropy},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2010},
}

@inproceedings{kitani1,
  author    = {D. Huang and
               K. M. Kitani},
  title     = {Action-Reaction: Forecasting the Dynamics of Human Interaction},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2014},
}

@inproceedings{kitani2,
  author    = {D. Huang and
               A. Farahmand and
               K. M. Kitani and
               J. A. Bagnell},
  title     = {Approximate {MaxEnt} Inverse Optimal Control and Its Application for
               Mental Simulation of Human Interactions},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year      = {2015},
}

@article{realnvp,
title={Latent Space Policies for Hierarchical Reinforcement Learning},
author={T. Haarnoja and K. Hartikainen and P. Abbeel and S. Levine},
journal   = {CoRR},
volume    = {abs/1804.02808},
year      = {2018},
}

@inproceedings{Javdani, 
    AUTHOR    = {S. Javdani and S. Srinivasa and J. A. Bagnell}, 
    TITLE     = {Shared Autonomy via Hindsight Optimization}, 
    BOOKTITLE = {Robotics: Science and Systems (RSS)}, 
    YEAR      = {2015}, 
}

@inproceedings{airl,
title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
author={Fu, J. and Luo, K. and Levine, S.},
}

@inproceedings{hausman,
title={Learning an Embedding Space for Transferable Robot Skills},
author={K. Hausman and J. T. Springenberg and Z. Wang and N. Heess and M. Riedmiller},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
}

@article{learning_to_explore,
  author    = {A. Gupta and
               R. Mendonca and
               Y. Liu and
               P. Abbeel and
               S. Levine},
  title     = {Meta-Reinforcement Learning of Structured Exploration Strategies},
  journal   = {CoRR},
  volume    = {abs/1802.07245},
  year      = {2018},
}

@article{endtoend,
 author = {Levine, S. and Finn, C. and Darrell, T. and Abbeel, P.},
 title = {End-to-end Training of Deep Visuomotor Policies},
 journal = {Journal of Machine Learning Research},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
} 

@inproceedings{cgps,
    author = {Sergey Levine and Vladlen Koltun},
    title = {Learning Complex Neural Network Policies with Trajectory Optimization},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2014},
}
@inproceedings{schulman,
title = {Equivalence Between Policy Gradients and Soft Q-Learning},
author  = {J. Schulman and X. Chen and P. Abbeel},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1704.06440}
}

@inproceedings{nvil,
  author    = {A. Mnih and
               K. Gregor},
  title     = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2014}
}

@inproceedings{gcl,
  author    = {Chelsea Finn and
               Sergey Levine and
               Pieter Abbeel},
  title     = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
    booktitle   = {International Conference on Machine Learning (ICML)},
  year      = {2016},
}

@inproceedings{gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{gail,
title = {Generative Adversarial Imitation Learning},
author = {Ho, J. and Ermon, S.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2016},
}

@article{finn_adversarial,
  author    = {Chelsea Finn and
               Paul Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
}

@inproceedings{bornschein2016bidirectional,
  title={Bidirectional helmholtz machines},
  author={Bornschein, Jorg and Shabanian, Samira and Fischer, Asja and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2511--2519},
  year={2016}
}

@article{marino2018iterative,
  title={Iterative amortized inference},
  author={Marino, Joseph and Yue, Yisong and Mandt, Stephan},
  journal={arXiv preprint arXiv:1807.09356},
  year={2018}
}

@inproceedings{rasmus2015semi,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  booktitle={Advances in neural information processing systems},
  pages={3546--3554},
  year={2015}
}

@inproceedings{sonderby2016ladder,
  title={Ladder variational autoencoders},
  author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={Advances in neural information processing systems},
  pages={3738--3746},
  year={2016}
}

@inproceedings{zhao2017learning,
  title={Learning hierarchical features from deep generative models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={4091--4099},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{hsu2017unsupervised,
  title={Unsupervised learning of disentangled and interpretable representations from sequential data},
  author={Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  booktitle={Advances in neural information processing systems},
  pages={1878--1889},
  year={2017}
}


@inproceedings{kalashnikov2018qtopt,
  title={Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018}
}

@article{cabi2019framework,
  title={A Framework for Data-Driven Robotics},
  author={Cabi, Serkan and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and {\.Z}o{\l}na, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and others},
  journal={arXiv preprint arXiv:1909.12200},
  year={2019}
}

 @article{Mo18AdobeIndoorNav,
        Author = {Kaichun Mo and Haoxiang Li and Zhe Lin and Joon-Young Lee},
        Title = {The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation},
        Year = {2018},
        Eprint = {arXiv:1802.08824},
    }
    
@article{kandasamy2017batch,
  title={Batch policy gradient methods for improving neural conversation models},
  author={Kandasamy, Kirthevasan and Bachrach, Yoram and Tomioka, Ryota and Tarlow, Daniel and Carter, David},
  journal={arXiv preprint arXiv:1702.03334},
  year={2017}
}

@inproceedings{sun2018fast,
  title={A fast integrated planning and control framework for autonomous driving via imitation learning},
  author={Sun, Liting and Peng, Cheng and Zhan, Wei and Tomizuka, Masayoshi},
  booktitle={Dynamic Systems and Control Conference},
  volume={51913},
  pages={V003T37A012},
  year={2018},
  organization={American Society of Mechanical Engineers}
}

@article{zhou2017end,
  title={End-to-end offline goal-oriented dialog policy learning via policy gradient},
  author={Zhou, Li and Small, Kevin and Rokhlenko, Oleg and Elkan, Charles},
  journal={arXiv preprint arXiv:1712.02838},
  year={2017}
}

@article{pan2017agile,
  title={Agile autonomous driving using end-to-end deep imitation learning},
  author={Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  journal={arXiv preprint arXiv:1709.07174},
  year={2017}
}

@article{nie2019learning,
  title={Learning when-to-treat policies},
  author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
  journal={arXiv preprint arXiv:1905.09751},
  year={2019}
}

@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@article{yu2018bdd100k,
  title={Bdd100k: A diverse driving video database with scalable annotation tooling},
  author={Yu, Fisher and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Liao, Mike and Madhavan, Vashisht and Darrell, Trevor},
  journal={arXiv preprint arXiv:1805.04687},
  year={2018}
}

@article{sallab2017deep,
  title={Deep reinforcement learning framework for autonomous driving},
  author={Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  journal={Electronic Imaging},
  volume={2017},
  number={19},
  pages={70--76},
  year={2017},
  publisher={Society for Imaging Science and Technology}
}

@article{yurtsever2020survey,
  title={A survey of autonomous driving: Common practices and emerging technologies},
  author={Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  journal={IEEE Access},
  year={2020},
  publisher={IEEE}
}

@inproceedings{kendall2019learning,
  title={Learning to drive in a day},
  author={Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8248--8254},
  year={2019},
  organization={IEEE}
}

@article{maddern2017robotcar,
  title={1 year, 1000 km: The Oxford RobotCar dataset},
  author={Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
  journal={The International Journal of Robotics Research},
  volume={36},
  number={1},
  pages={3--15},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional imitation learning},
  author={Codevilla, Felipe and Miiller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--9},
  year={2018},
  organization={IEEE}
}

@inproceedings{tassa2012synthesis,
  title={Synthesis and stabilization of complex behaviors through online trajectory optimization},
  author={Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={4906--4913},
  year={2012},
  organization={IEEE}
}

@inproceedings{pets,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4754--4765},
  year={2018}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE}
}

@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{pietquin2011sample,
  title={Sample-efficient batch reinforcement learning for dialogue management optimization},
  author={Pietquin, Olivier and Geist, Matthieu and Chandramohan, Senthilkumar and Frezza-Buet, Herv{\'e}},
  journal={ACM Transactions on Speech and Language Processing (TSLP)},
  volume={7},
  number={3},
  pages={1--21},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nat Med},
  volume={25},
  number={1},
  pages={16--18},
  year={2019}
}

@inproceedings{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={advances in neural information processing systems},
  pages={3231--3239},
  year={2015}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  number={28},
  year={1970},
  publisher={Princeton university press}
}

@article{nachum2020reinforcement,
  title={Reinforcement Learning via Fenchel-Rockafellar Duality},
  author={Nachum, Ofir and Dai, Bo},
  journal={arXiv preprint arXiv:2001.01866},
  year={2020}
}

@inproceedings{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2315--2325},
  year={2019}
}

@article{eysenbach2017leave,
  title={Leave no trace: Learning to reset for safe and autonomous reinforcement learning},
  author={Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1711.06782},
  year={2017}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@inproceedings{osband2018randomized,
  title={Randomized prior functions for deep reinforcement learning},
  author={Osband, Ian and Aslanides, John and Cassirer, Albin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8617--8629},
  year={2018}
}


@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017}
}

@article{achiam2019towards,
  title={Towards characterizing divergence in deep q-learning},
  author={Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1903.08894},
  year={2019}
}

@article{gupta2019relay,
  title={Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:1910.11956},
  year={2019}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@article{lagoudakis2003least,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={Journal of machine learning research},
  volume={4},
  number={Dec},
  pages={1107--1149},
  year={2003}
}

@article{sutton1991dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011}
}

@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Li-wei, H Lehman and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  pages={160035},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{guez2008adaptive,
  title={Adaptive Treatment of Epilepsy via Batch-mode Reinforcement Learning.},
  author={Guez, Arthur and Vincent, Robert D and Avoli, Massimo and Pineau, Joelle},
  booktitle={AAAI},
  pages={1671--1678},
  year={2008}
}

@article{pipps,
  title={PIPPS: Flexible model-based policy search robust to the curse of chaos},
  author={Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  journal={arXiv preprint arXiv:1902.01240},
  year={2019}
}

@article{simpl,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@inproceedings{mbpo,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12498--12509},
  year={2019}
}

@article{raghu2017deep,
  title={Deep reinforcement learning for sepsis treatment},
  author={Raghu, Aniruddh and Komorowski, Matthieu and Ahmed, Imran and Celi, Leo and Szolovits, Peter and Ghassemi, Marzyeh},
  journal={arXiv preprint arXiv:1711.09602},
  year={2017}
}

@article{prasad2017reinforcement,
  title={A reinforcement learning approach to weaning of mechanical ventilation in intensive care units},
  author={Prasad, Niranjani and Cheng, Li-Fang and Chivers, Corey and Draugelis, Michael and Engelhardt, Barbara E},
  journal={arXiv preprint arXiv:1704.06300},
  year={2017}
}

@inproceedings{swaminathan2017off,
  title={Off-policy evaluation for slate recommendation},
  author={Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dudik, Miro and Langford, John and Jose, Damien and Zitouni, Imed},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3632--3642},
  year={2017}
}

@inproceedings{osband2017posterior,
  title={Why is posterior sampling better than optimism for reinforcement learning?},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2701--2710},
  year={2017},
  organization={JMLR. org}
}

@article{peng2019awr,
  title={Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@article{sriperumbudur2009integral,
  title={On integral probability metrics,$\backslash$phi-divergences and binary classification},
  author={Sriperumbudur, Bharath K and Fukumizu, Kenji and Gretton, Arthur and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={arXiv preprint arXiv:0901.2698},
  year={2009}
}

@article{fujimoto2018off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={arXiv preprint arXiv:1812.02900},
  year={2018}
}

@inproceedings{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11761--11771},
  year={2019}
}

@article{siegel2020keep,
  title={Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}

@inproceedings{nowozin2016f,
  title={f-gan: Training generative neural samplers using variational divergence minimization},
  author={Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  booktitle={Advances in neural information processing systems},
  pages={271--279},
  year={2016}
}

@article{wu2019domain,
  title={Domain adaptation with asymmetrically-relaxed distribution alignment},
  author={Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  journal={arXiv preprint arXiv:1903.01689},
  year={2019}
}

@inproceedings{sutton2009fastgtd,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={993--1000},
  year={2009}
}


@article{alphastar,
  title={Mastering the Real-Time Strategy Game StarCraft II},
  author={AlphaStar, DeepMind},
  journal={URL: https://deepmind. com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii}
}

@article{lu2018general,
  title={A General Family of Robust Stochastic Operators for Reinforcement Learning},
  author={Lu, Yingdong and Squillante, Mark S and Wu, Chai Wah},
  journal={arXiv preprint arXiv:1805.08122},
  year={2018}
}

@inproceedings{
Zhang2020GenDICE:,
title={GenDICE: Generalized Offline Estimation of Stationary Values},
author={Ruiyi Zhang and Bo Dai and Lihong Li and Dale Schuurmans},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkxlcnVFwB}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{bellemare2016increasing,
  title={Increasing the action gap: New operators for reinforcement learning},
  author={Bellemare, Marc G and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S and Munos, R{\'e}mi},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{rajeswaran2018dapg,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  booktitle={Robotics: Science and Systems},
  year={2018}
}

@inproceedings{hester2018deep,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@misc{
sachdeva2020offpolicy,
title={Off-policy Bandits with Deficient Support},
author={Noveen Sachdeva and Yi Su and Thorsten Joachims},
year={2020},
url={https://openreview.net/forum?id=SklcyJBtvB}
}

@inproceedings{li2010contextual,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010}
}

@article{o2018variational,
  title={Variational Bayesian reinforcement learning with regret bounds},
  author={O'Donoghue, Brendan},
  journal={arXiv preprint arXiv:1807.09647},
  year={2018}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@InProceedings{fujimoto2018addressing,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {1587--1596},
  year = 	 {2018},
}

@misc{d4rl_repo,
title = {D4RL: Datasets for Data-Driven Deep Reinforcement Learning},
author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
note = {Github repository},
year = {2020},
howpublished  = {\url{https://github.com/rail-berkeley/d4rl/wiki/New-Franka-Kitchen-Tasks}}
}



@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{furuichi2004fundamental,
  title={Fundamental properties of Tsallis relative entropy},
  author={Furuichi, Shigeru and Yanagi, Kenjiro and Kuriyama, Ken},
  journal={Journal of Mathematical Physics},
  volume={45},
  number={12},
  pages={4868--4877},
  year={2004},
  publisher={American Institute of Physics}
}

@inproceedings{namkoong2017variance,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in neural information processing systems},
  pages={2971--2980},
  year={2017}
}

@inproceedings{tamar2014scaling,
  title={Scaling up robust MDPs using function approximation},
  author={Tamar, Aviv and Mannor, Shie and Xu, Huan},
  booktitle={International Conference on Machine Learning},
  pages={181--189},
  year={2014}
}

@article{nair2020accelerating,
  title={Accelerating Online Reinforcement Learning with Offline Datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@inproceedings{nilim2004robustness,
  title={Robustness in Markov decision problems with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  booktitle={Advances in neural information processing systems},
  pages={839--846},
  year={2004}
}

@article{russel2018tight,
  title={Tight bayesian ambiguity sets for robust MDPs},
  author={Russel, Reazul Hasan and Petrik, Marek},
  journal={arXiv preprint arXiv:1811.06512},
  year={2018}
}

@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@inproceedings{ghavamzadeh2016safe,
  title={Safe policy improvement by minimizing robust baseline regret},
  author={Petrik, Marek and Ghavamzadeh, Mohammad and Chow, Yinlam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2298--2306},
  year={2016}
}

@inproceedings{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2217--2225},
  year={2010}
}

@article{simao2019safe,
  title={Safe Policy Improvement with an Estimated Baseline Policy},
  author={Sim{\~a}o, Thiago D and Laroche, Romain and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1909.05236},
  year={2019}
}

@article{laroche2017safe,
  title={Safe policy improvement with baseline bootstrapping},
  author={Laroche, Romain and Trichelair, Paul and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1712.06924},
  year={2017}
}

@article{nachum2019algaedice,
  title={AlgaeDICE: Policy Gradient from Arbitrary Experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@article{brac,
  title={Behavior Regularized Offline Reinforcement Learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@inproceedings{o2018uncertainty,
  title={The Uncertainty Bellman Equation and Exploration},
  author={Oâ€™Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={3836--3845},
  year={2018}
}

@inproceedings{gilotte2018offline,
  title={Offline a/b testing for recommender systems},
  author={Gilotte, Alexandre and Calauz{\`e}nes, Cl{\'e}ment and Nedelec, Thomas and Abraham, Alexandre and Doll{\'e}, Simon},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={198--206},
  year={2018}
}

@article{bottou2013counterfactual,
  title={Counterfactual reasoning and learning systems: The example of computational advertising},
  author={Bottou, L{\'e}on and Peters, Jonas and Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3207--3260},
  year={2013},
  publisher={JMLR. org}
}

@inproceedings{theocharous2015personalized,
  title={Personalized ad recommendation systems for life-time value optimization with guarantees},
  author={Theocharous, Georgios and Thomas, Philip S and Ghavamzadeh, Mohammad},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}


@article{henderson2008hybrid,
  title={Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets},
  author={Henderson, James and Lemon, Oliver and Georgila, Kallirroi},
  journal={Computational Linguistics},
  volume={34},
  number={4},
  pages={487--511},
  year={2008},
  publisher={MIT Press}
}


@inproceedings{tesauro2006hybrid,
  title={A hybrid reinforcement learning approach to autonomic resource allocation},
  author={Tesauro, Gerald and Jong, Nicholas K and Das, Rajarshi and Bennani, Mohamed N},
  booktitle={2006 IEEE International Conference on Autonomic Computing},
  pages={65--73},
  year={2006},
  organization={IEEE}
}

@inproceedings{nazari2018reinforcement,
  title={Reinforcement learning for solving the vehicle routing problem},
  author={Nazari, Mohammadreza and Oroojlooy, Afshin and Snyder, Lawrence and Tak{\'a}c, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9839--9849},
  year={2018}
}

@article{balaji2010urban,
  title={Urban traffic signal control using reinforcement learning agents},
  author={Balaji, PG and German, X and Srinivasan, Dipti},
  journal={IET Intelligent Transport Systems},
  volume={4},
  number={3},
  pages={177--188},
  year={2010},
  publisher={IET}
}

@inproceedings{hein2017industrial,
  title={Batch reinforcement learning on the industrial benchmark: First experiences},
  author={Hein, Daniel and Udluft, Steffen and Tokic, Michel and Hentschel, Alexander and Runkler, Thomas A and Sterzing, Volkmar},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={4214--4221},
  year={2017},
  organization={IEEE}
}

@article{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@article{fu2019diagnosing,
  title={Diagnosing bottlenecks in deep {Q}-learning algorithms},
  author={Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  journal={arXiv preprint arXiv:1902.10250},
  year={2019}
}

@inproceedings{imani2018off,
  title={An off-policy policy gradient theorem using emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={96--106},
  year={2018}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{gu2017interpolated,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in neural information processing systems},
  pages={3846--3855},
  year={2017}
}

@inproceedings{gelada2019off,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3647--3655},
  year={2019}
}

@article{gabel2008adaptive,
  title={Adaptive reactive job-shop scheduling with reinforcement learning agents},
  author={Gabel, Thomas and Riedmiller, Martin},
  journal={International Journal of Information Technology and Intelligent Computing},
  volume={24},
  number={4},
  pages={14--18},
  year={2008},
  publisher={Citeseer}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{peshkin2002learning,
  title={Learning from scarce experience},
  author={Peshkin, Leonid and Shelton, Christian R},
  journal={arXiv preprint cs/0204043},
  year={2002}
}

@inproceedings{precup2001off,
  title={Off-policy temporal-difference learning with function approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={ICML},
  pages={417--424},
  year={2001}
}

@inproceedings{hallak2017consistent,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1372--1383},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016}
}

@inproceedings{wang2017optimal,
  title={Optimal and adaptive off-policy evaluation in contextual bandits},
  author={Wang, Yu-Xiang and Agarwal, Alekh and Dudik, Miroslav},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3589--3597},
  year={2017},
  organization={JMLR. org}
}

@article{jiang2015doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  journal={arXiv preprint arXiv:1511.03722},
  year={2015}
}

@article{farajtabar2018more,
  title={More robust doubly robust off-policy evaluation},
  author={Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1802.03493},
  year={2018}
}

@inproceedings{thomas2015high,
  title={High confidence policy improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={2380--2388},
  year={2015}
}


@inproceedings{scherrer2014approximate,
  title={Approximate policy iteration schemes: a comparison},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={1314--1322},
  year={2014}
}


@article{xie2020q,
  title={Q* Approximation Schemes for Batch Reinforcement Learning: A eoretical Comparison},
  author={Xie, Tengyang and Jiang, Nan},
  year={2020}
}

@inproceedings{hanna2017bootstrapping,
  title={Bootstrapping with models: Confidence intervals for off-policy evaluation},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{thomas2015higheval,
  title={High-confidence off-policy evaluation},
  author={Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}


@article{gu2016q,
  title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}

@article{nadjahi2019safe,
  title={Safe Policy Improvement with Soft Baseline Bootstrapping},
  author={Nadjahi, Kimia and Laroche, Romain and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1907.05079},
  year={2019}
}

@article{cheng2019trajectory,
  title={Trajectory-wise control variates for variance reduction in policy gradient methods},
  author={Cheng, Ching-An and Yan, Xinyan and Boots, Byron},
  journal={arXiv preprint arXiv:1908.03263},
  year={2019}
}

@article{pankov2018reward,
  title={Reward-estimation variance elimination in sequential decision processes},
  author={Pankov, Sergey},
  journal={arXiv preprint arXiv:1811.06225},
  year={2018}
}

@article{huang2019importance,
  title={From Importance Sampling to Doubly Robust Policy Gradient},
  author={Huang, Jiawei and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.09066},
  year={2019}
}

@article{sutton2016emphatic,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{yu2015convergence,
  title={On convergence of emphatic temporal-difference learning},
  author={Yu, Huizhen},
  booktitle={Conference on Learning Theory},
  pages={1724--1751},
  year={2015}
}

@inproceedings{hallak2016generalized,
  title={Generalized emphatic temporal difference learning: Bias-variance analysis},
  author={Hallak, Assaf and Tamar, Aviv and Munos, R{\'e}mi and Mannor, Shie},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{zhang2019generalized,
  title={Generalized off-policy actor-critic},
  author={Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1999--2009},
  year={2019}
}

@article{hallak2015emphatic,
  title={Emphatic TD Bellman Operator is a Contraction},
  author={Hallak, Assaf and Tamar, Aviv and Mannor, Shie},
  journal={arXiv preprint arXiv:1508.03411},
  year={2015}
}

@inproceedings{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5356--5366},
  year={2018}
}

@article{liu2019off,
  title={Off-policy policy gradient with state distribution correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:1904.08473},
  year={2019}
}

@article{mousavi2020black,
  title={Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning},
  author={Mousavi, Ali and Li, Lihong and Liu, Qiang and Zhou, Denny},
  journal={arXiv preprint arXiv:2003.11126},
  year={2020}
}

@article{kallus2019efficiently,
  title={Efficiently breaking the curse of horizon: Double reinforcement learning in infinite-horizon processes},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:1909.05850},
  year={2019}
}

@inproceedings{kallus2019intrinsically,
  title={Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning},
  author={Kallus, Nathan and Uehara, Masatoshi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3320--3329},
  year={2019}
}

@article{uehara2019minimax,
  title={Minimax Weight and Q-Function Learning for Off-Policy Evaluation},
  author={Uehara, Masatoshi and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.12809},
  year={2019}
}

@article{tang2019doubly,
  title={Doubly robust bias reduction in infinite horizon off-policy estimation},
  author={Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
  journal={arXiv preprint arXiv:1910.07186},
  year={2019}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{dai2017boosting,
  title={Boosting the actor with dual critic},
  author={Dai, Bo and Shaw, Albert and He, Niao and Li, Lihong and Song, Le},
  journal={arXiv preprint arXiv:1712.10282},
  year={2017}
}

@article{wang2021instabilities,
  title={Instabilities of Offline RL with Pre-Trained Neural Representation},
  author={Wang, Ruosong and Wu, Yifan and Salakhutdinov, Ruslan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2103.04947},
  year={2021}
}

@article{kostrikov2021offline,
  title={Offline Reinforcement Learning with Fisher Divergence Critic Regularization},
  author={Kostrikov, Ilya and Tompson, Jonathan and Fergus, Rob and Nachum, Ofir},
  journal={arXiv preprint arXiv:2103.08050},
  year={2021}
}

@article{jin2020pessimism,
  title={Is Pessimism Provably Efficient for Offline RL?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2012.15085},
  year={2020}
}

@article{dai2017sbeed,
  title={SBEED: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  journal={arXiv preprint arXiv:1712.10285},
  year={2017}
}

@article{chen2016stochastic,
  title={Stochastic primal-dual methods and sample complexity of reinforcement learning},
  author={Chen, Yichen and Wang, Mengdi},
  journal={arXiv preprint arXiv:1612.02516},
  year={2016}
}

@inproceedings{wang2016online,
  title={An online primal-dual method for discounted markov decision processes},
  author={Wang, Mengdi and Chen, Yichen},
  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages={4516--4521},
  year={2016},
  organization={IEEE}
}

@inproceedings{
kumar2021implicit,
title={Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning},
author={Aviral Kumar and Rishabh Agarwal and Dibya Ghosh and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=O9bnihsFfXU}
}

@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{durugkar2018td,
  title={TD learning with constrained gradients},
  author={Durugkar, Ishan and Stone, Peter},
  year={2018}
}

@article{singh2020cog,
  title={COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning},
  author={Singh, Avi and Yu, Albert and Yang, Jonathan and Zhang, Jesse and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.14500},
  year={2020}
}

@inproceedings{dong2020expressivity,
  title={On the expressivity of neural networks for deep reinforcement learning},
  author={Dong, Kefan and Luo, Yuping and Yu, Tianhe and Finn, Chelsea and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={2627--2637},
  year={2020},
  organization={PMLR}
}

@inproceedings{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2667--2690},
  year={2019},
  organization={PMLR}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{
kumar2021workflow,
title={A Workflow for Offline Model-Free Robotic Reinforcement Learning},
author={Aviral Kumar and Anikait Singh and Stephen Tian and Chelsea Finn and Sergey Levine},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=fy4ZBWxYbIo}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{ghiassian2020gradient,
  title={Gradient temporal-difference learning with regularized corrections},
  author={Ghiassian, Sina and Patterson, Andrew and Garg, Shivam and Gupta, Dhawal and White, Adam and White, Martha},
  booktitle={International Conference on Machine Learning},
  pages={3524--3534},
  year={2020},
  organization={PMLR}
}

@inproceedings{gogianu2021spectral,
  title={Spectral normalisation for deep reinforcement learning: an optimisation perspective},
  author={Gogianu, Florin and Berariu, Tudor and Rosca, Mihaela C and Clopath, Claudia and Busoniu, Lucian and Pascanu, Razvan},
  booktitle={International Conference on Machine Learning},
  pages={3734--3744},
  year={2021},
  organization={PMLR}
}

@article{lee2021versions,
  title={Versions of Gradient Temporal Difference Learning},
  author={Lee, Donghwan and Lim, Han-Dong and Park, Jihoon and Choi, Okyong},
  journal={arXiv preprint arXiv:2109.04033},
  year={2021}
}

@article{cheng2022adversarially,
  title={Adversarially Trained Actor Critic for Offline Reinforcement Learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{kostrikov2020image,
  title={Image augmentation is all you need: Regularizing deep reinforcement learning from pixels},
  author={Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  journal={arXiv preprint arXiv:2004.13649},
  year={2020}
}

@article{kumar2021dr3,
  title={{DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization}},
  author={Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2112.04716},
  year={2021}
}

@article{pohlen2018observe,
  title={Observe and look further: Achieving consistent performance on atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and Van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'\i}k, Mel and others},
  journal={arXiv preprint arXiv:1805.11593},
  year={2018}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zanette2020exponential,
  title={Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL},
  author={Zanette, Andrea},
  journal={arXiv preprint arXiv:2012.08005},
  year={2020}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}

@inproceedings{
wang2021what,
title={What are the Statistical Limits of Offline {\{}RL{\}} with Linear Function Approximation?},
author={Ruosong Wang and Dean Foster and Sham M. Kakade},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=30EvkP2aQLD}
}

@article{liu2020provably,
  title={Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{duan2020minimax,
  title={Minimax-optimal off-policy evaluation with linear function approximation},
  author={Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={2701--2709},
  year={2020},
  organization={PMLR}
}

@article{lee2018stochastic,
  title={Stochastic primal-dual Q-learning},
  author={Lee, Donghwan and He, Niao},
  journal={arXiv preprint arXiv:1810.08298},
  year={2018}
}

@inproceedings{swaminathan2015counterfactual,
  title={Counterfactual risk minimization: Learning from logged bandit feedback},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={International Conference on Machine Learning},
  pages={814--823},
  year={2015}
}

@article{hayes2005large,
  title={A large-deviation inequality for vector-valued martingales},
  author={Hayes, Thomas P},
  journal={Combinatorics, Probability and Computing},
  year={2005}
}


@misc{GoodfellowPMXWOCB14,
  doi = {10.48550/ARXIV.1406.2661},
  
  url = {https://arxiv.org/abs/1406.2661},
  
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{sutton1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@article{MBRL,
  author    = {Quan Vuong and
               Shuang Liu and
               Minghua Liu and
               Kamil Ciosek and
               Hao Su and
               Henrik Iskov Christensen},
  title     = {Multi-task Batch Reinforcement Learning with Metric Learning},
  journal   = {CoRR},
  volume    = {abs/1909.11373},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11373},
  eprinttype = {arXiv},
  eprint    = {1909.11373},
  timestamp = {Wed, 11 Nov 2020 08:48:08 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11373.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{mtopt,
  author    = {Dmitry Kalashnikov and
               Jacob Varley and
               Yevgen Chebotar and
               Benjamin Swanson and
               Rico Jonschkowski and
               Chelsea Finn and
               Sergey Levine and
               Karol Hausman},
  title     = {MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/2104.08212},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08212},
  eprinttype = {arXiv},
  eprint    = {2104.08212},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08212.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Snell2022,
  doi = {10.48550/ARXIV.2204.10198},
  
  url = {https://arxiv.org/abs/2204.10198},
  
  author = {Snell, Charlie and Yang, Mengjiao and Fu, Justin and Su, Yi and Levine, Sergey},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Context-Aware Language Modeling for Goal-Oriented Dialogue Systems},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{kumar2022prefer,
      title={When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?}, 
      author={Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},
      year={2022},
      eprint={2204.05618},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bcq,
  doi = {10.48550/ARXIV.1812.02900},
  
  url = {https://arxiv.org/abs/1812.02900},
  
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Off-Policy Deep Reinforcement Learning without Exploration},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{BAIL,
  doi = {10.48550/ARXIV.1910.12179},
  
  url = {https://arxiv.org/abs/1910.12179},
  
  author = {Chen, Xinyue and Zhou, Zijian and Wang, Zheng and Wang, Che and Wu, Yanqiu and Ross, Keith},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
furuta2022generalized,
title={Generalized Decision Transformer for Offline Hindsight Information Matching},
author={Hiroki Furuta and Yutaka Matsuo and Shixiang Shane Gu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=CAjxVodl_v}
}


@inproceedings{
jang2022gptcritic,
title={{GPT}-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems},
author={Youngsoo Jang and Jongmin Lee and Kee-Eung Kim},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=qaxhBG1UUaS}
}

@misc{
meng2022offline,
title={Offline Pre-trained Multi-Agent Decision Transformer},
author={Linghui Meng and Muning Wen and Yaodong Yang and chenyang le and Xi yun Li and Haifeng Zhang and Ying Wen and Weinan Zhang and Jun Wang and Bo XU},
year={2022},
url={https://openreview.net/forum?id=W08IqLMlMer}
}

@misc{
daoudi2022density,
title={Density Estimation for Conservative Q-Learning},
author={Paul Daoudi and Merwan Barlier and Ludovic Dos Santos and Aladin Virmaux},
year={2022},
url={https://openreview.net/forum?id=liV-Re74fK}
}

@misc{
liu2022robust,
title={Robust Imitation Learning from Corrupted Demonstrations},
author={Liu Liu and Ziyang Tang and Lanqing Li and Dijun Luo},
year={2022},
url={https://openreview.net/forum?id=UECzHrGio7i}
}

@book{10.5555/561828,
author = {Murray, Richard M. and Sastry, S. Shankar and Zexiang, Li},
title = {A Mathematical Introduction to Robotic Manipulation},
year = {1994},
isbn = {0849379814},
publisher = {CRC Press, Inc.},
address = {USA},
edition = {1st}
}

@article{zhihanliu,
  author    = {Zhihan Liu and
               Yufeng Zhang and
               Zuyue Fu and
               Zhuoran Yang and
               Zhaoran Wang},
  title     = {Provably Efficient Generative Adversarial Imitation Learning for Online
               and Offline Setting with Linear Function Approximation},
  journal   = {CoRR},
  volume    = {abs/2108.08765},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.08765},
  eprinttype = {arXiv},
  eprint    = {2108.08765},
  timestamp = {Fri, 29 Apr 2022 19:49:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-08765.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{intentiongan,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Hausman, Karol and Chebotar, Yevgen and Schaal, Stefan and Sukhatme, Gaurav S. and Lim, Joseph J.},
  biburl = {https://www.bibsonomy.org/bibtex/2083333504c56125bca126e125ddfb765/dblp},
  booktitle = {NeurIPS},
  crossref = {conf/nips/2017},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  ee = {http://papers.nips.cc/paper/6723-multi-modal-imitation-learning-from-unstructured-demonstrations-using-generative-adversarial-nets},
  interhash = {db1ed5bf41488901ae91494683988da9},
  intrahash = {083333504c56125bca126e125ddfb765},
  keywords = {dblp},
  pages = {1235-1245},
  timestamp = {2020-03-07T11:55:59.000+0100},
  title = {Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets.},
  year = 2017
}

@inproceedings{infogail,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},
  biburl = {https://www.bibsonomy.org/bibtex/2ddaff729a6f5bdf65ca5b8a92c687721/dblp},
  booktitle = {NeurIPS},
  crossref = {conf/nips/2017},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  ee = {http://papers.nips.cc/paper/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations},
  interhash = {cb317cd23cb7dade0d27a866baeb17e5},
  intrahash = {ddaff729a6f5bdf65ca5b8a92c687721},
  keywords = {dblp},
  pages = {3812-3822},
  timestamp = {2020-03-07T11:55:59.000+0100},
  title = {InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations.},
  year = 2017
}



@misc{paszke2019pytorch,
  abstract = {Deep learning frameworks have often focused on either usability or speed, but
not both. PyTorch is a machine learning library that shows that these two goals
are in fact compatible: it provides an imperative and Pythonic programming
style that supports code as a model, makes debugging easy and is consistent
with other popular scientific computing libraries, while remaining efficient
and supporting hardware accelerators such as GPUs.
  In this paper, we detail the principles that drove the implementation of
PyTorch and how they are reflected in its architecture. We emphasize that every
aspect of PyTorch is a regular Python program under the full control of its
user. We also explain how the careful and pragmatic implementation of the key
components of its runtime enables them to work together to achieve compelling
performance.
  We demonstrate the efficiency of individual subsystems, as well as the
overall speed of PyTorch on several common benchmarks.},
  added-at = {2020-01-10T22:19:45.000+0100},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and KÃ¶pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  biburl = {https://www.bibsonomy.org/bibtex/2018e9538aaad774ee16f704b858a78dd/analyst},
  description = {[1912.01703] PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  interhash = {a3caca5456f48ad236ed06df5cf0ecca},
  intrahash = {018e9538aaad774ee16f704b858a78dd},
  keywords = {2019 deep-learning facebook library pytorch},
  note = {cite arxiv:1912.01703Comment: 12 pages, 3 figures, NeurIPS 2019},
  timestamp = {2021-04-21T16:03:01.000+0200},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url = {http://arxiv.org/abs/1912.01703},
  year = 2019
}


@misc{https://doi.org/10.48550/arxiv.1610.04490,
  doi = {10.48550/ARXIV.1610.04490},
  
  url = {https://arxiv.org/abs/1610.04490},
  
  author = {SÃ¸nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and HuszÃ¡r, Ferenc},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Amortised MAP Inference for Image Super-resolution},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}


@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{lsgan,
  doi = {10.48550/ARXIV.1611.04076},
  
  url = {https://arxiv.org/abs/1611.04076},
  
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Least Squares Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gan_instance_noise,
  doi = {10.48550/ARXIV.1610.04490},
  
  url = {https://arxiv.org/abs/1610.04490},
  
  author = {SÃ¸nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and HuszÃ¡r, Ferenc},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Amortised MAP Inference for Image Super-resolution},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

