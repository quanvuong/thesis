\section{Preliminaries and Problem Statement}
\label{sec_prelim}

We model a task as a Markov Decision Process $M = (\mathcal{S}, \mathcal{A}, T, T_0, R, H)$, with state space $\mathcal{S}$, action space $\mathcal{A}$, transition function $T$, initial state distribution $T_0$, reward function $R$, and horizon $H$. At each discrete timestep $t$, the agent is in a state $s_t$, picks an action $a_t$, arrives at $s'_t\sim T(\cdot|s_t, a_t)$, and receives a reward $R(s_t, a_t, s'_t)$. The performance measure of policy $\pi$ is the expected sum of rewards $J_M(\pi) = \mathbb{E}_{\tau_M \sim \pi}[\sum_{t=0}^{H-1} R(s_t, a_t, s'_t)]$, where $\tau_M = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ is a trajectory generated by using $\pi$ to interact with $M$.


\subsection{Batch Reinforcement Learning}
\label{BCQ}

A Batch RL algorithm solves the task using an existing batch of $N$ transitions $\mathcal{B} = \{(s_{t}, a_{t}, r_{t}, s'_{t}) | t = 1, \ldots, N\}$. A recent advance in this area is Batch Constrained Q-Learning (BCQ) \cite{fujimoto2019off}. Here, we explain how BCQ selects actions. Given a state $s$, a generator $G$ outputs multiple candidate actions $\{a_m\}_{m}$. A perturbation model $\xi$ takes as input the state-candidate action and generates small correction $\xi(s, a_m)$. The corrected action with the highest estimated $Q$ value is selected as $\pi\left(s\right)$:
\begin{equation}
    \pi\left(s\right)=\underset{a_{m}+\xi\left(s, a_{m}\right)}{\arg \max } Q\left(s, a_{m}+\xi\left(s, a_{m}\right)\right), \quad \quad\left\{a_{m}=G\left(s, \nu_{m}\right)\right\}_{m}, \quad \quad \nu_{m} \sim \mathcal{N}(0,1).
\end{equation}
In our paper, we use BCQ as a routine. The take-away is that BCQ takes as input a batch of transitions $\mathcal{B} = \{(s_{t}, a_{t}, r_{t}, s'_{t}) | t = 1, \ldots, N\}$ and outputs three learned functions $Q, G, \xi$.

\subsection{Multi-task Batch Reinforcement Learning}
\label{bg_mtbrl}

Given $K$ batches, each containing $N$ transition tuples from one task,
$\mathcal{B}_{i}=\{(s_{i, t}, a_{i, t}, r_{i, t}, s_{i, t}^{\prime}) | i = 1, \ldots, K, t=1, \ldots, N \}$,
we define the Multi-task Batch RL problem as:
\begin{equation}\label{eq_obj}
    \underset{\theta}{\arg \max } \,\, J(\theta)=\mathbb{E}_{M_{i} \sim p\left(M\right)}\left[ J_{M_i}(\pi_\theta)\right],
\end{equation}
where an algorithm only has access to the $K$ batches and $J_{M_i}(\pi)$ is the performance of the policy $\pi$ in task $i$, i.e. $\mathbb{E}_{\tau_{M_i} \sim \pi}[\sum_{t=0}^{H-1} R(s_{i, t}, a_{i, t}, s'_{i, t})]$. $p(M)$ defines a task distribution. The subscript $i$ indexes the different tasks. The tasks
have the same state and action space and
only differ in the transition and reward functions \cite{zintgraf2020varibad}. A distribution over the transition and/or the reward functions therefore defines the task distribution. We measure performance by computing average returns over unseen tasks sampled from the same task distribution. The policy is not given identity of the unseen tasks before evaluation and must infer it from collected transitions.

In multi-task RL, we can use a task inference module $q_\phi$ to infer the task identity from a context set. The context set for a task $i$ consists of transitions from task $i$ and is denoted ${\bf c}_i$.
The task inference module $q_\phi$ takes ${\bf c}_i$ as input and outputs a posterior over the task identity. We sample a task identity ${\bf z}_i$ from the posterior and inputs it to the policy in addition to the state, i.e. $\pi(s, {\bf z}_i)$.
We model $q_\phi$ with the probabilistic and permutation-invariant architecture from \cite{rakelly2019efficient}. $q_\phi$ outputs the parameters of a diagonal Gaussian. For conciseness, we sometimes use the term policy to also refer to the task inference module. It should be clear from the context whether we are referring to $q_\phi$ or $\pi$.

We evaluate a policy on unseen tasks in two different scenarios: (1) Allowing the policy to collect a small number of interactions to infer $z$, we evaluate returns without further training, (2) Training the policy in the unseen task and collecting as much data as needed, we evaluate the amount of transitions the policy needs to collect to converge to the optimal performance.

We assume that each batch $\mathcal{B}_{i}$ contains data generated by a policy while learning to solve task $M_{i}$. Thus, if solving each task involve visiting different subspace of the state space, the different batches do not have significant overlap in their state-action visitation frequencies. This is illustrated in Fig. \ref{fig:cheat}.
